{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31b79a0-c051-4885-8701-db22d397bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer, AutoTokenizer, TrainingArguments\n",
    "from model.model import HatefulMemesData, HateMemeClassifier, compute_metrics\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer, VisualBertModel, VisualBertConfig\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4a6a40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateMemeClassifier(torch.nn.Module):\n",
    "    def __init__(self,fusion_method, visual_embedder='vit',wandb_run=None):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        fusion_method: 'concatenate' or 'weight_ensemble' or 'linear_weight_ensemble' or 'visualbert'\n",
    "        visual_embedder: 'vit'\n",
    "        \"\"\"\n",
    "        super(HateMemeClassifier, self).__init__()\n",
    "        self.fusion_method = fusion_method # 'concatenate' or 'weight_ensemble' or 'linear_weight_ensemble' or 'visualbert'\n",
    "        self.wandb_run=wandb_run\n",
    "\n",
    "        configuration = VisualBertConfig.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre',\n",
    "                                                hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1)\n",
    "        self.visualbert = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre', config=configuration)\n",
    "        if self.fusion_method != 'visualbert':\n",
    "            self.bertmodel = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        if visual_embedder=='vit':\n",
    "            self.embed_cls = nn.Linear(768, 1024)\n",
    "        self.num_labels = 2\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "        if self.fusion_method=='weight_ensemble':\n",
    "            self.alpha = nn.Parameter(torch.tensor(0.5))  # Initial value of alpha\n",
    "            \n",
    "        if self.fusion_method=='linear_weight_ensemble':\n",
    "            self.alpha = nn.Parameter(torch.tensor(0.5))  # Initial value of alpha\n",
    "            self.cls_visual = nn.Sequential(\n",
    "                nn.Linear(768, 768),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.BatchNorm1d(768),\n",
    "            )\n",
    "            self.cls_text = nn.Sequential(\n",
    "                nn.Linear(768, 768),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.BatchNorm1d(768),\n",
    "            )\n",
    "\n",
    "        self.cls=nn.Linear(768, self.num_labels)\n",
    "\n",
    "        # Calculate the weights for the loss function and weight balanced loss\n",
    "        nSamples = [5450,3050]\n",
    "        normedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n",
    "        self.loss_fct = CrossEntropyLoss(weight=torch.FloatTensor(normedWeights))\n",
    "\n",
    "        # self.loss_fct = CrossEntropyLoss()\n",
    "        \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask,\n",
    "                visual_token_type_ids, labels,caption_input_ids, caption_attention_mask, caption_token_type_ids):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        visual_embeds_cls = self.embed_cls(visual_embeds)\n",
    "        \n",
    "        outputs = self.visualbert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                visual_embeds=visual_embeds_cls,\n",
    "                visual_attention_mask=visual_attention_mask,\n",
    "                visual_token_type_ids=visual_token_type_ids,\n",
    "            )\n",
    "        \n",
    "        visualbert_embedding = outputs[1] # output is a context vector of 768 dimensions\n",
    "\n",
    "        if self.fusion_method != \"visualbert\":\n",
    "        \n",
    "            caption_outputs = self.bertmodel(caption_input_ids, attention_mask=caption_attention_mask, token_type_ids=caption_token_type_ids)\n",
    "                    \n",
    "            # Get the embeddings of the [CLS] token\n",
    "            caption_embeddings = caption_outputs.last_hidden_state[:,0,:] # output is a context vector of 768 dimensions\n",
    "        \n",
    "        if self.fusion_method=='weight_ensemble':\n",
    "            # funsion model: weight ensenble of the two embeddings: alpha*visualbert_embedding + (1-alpha)*caption_embeddings \n",
    "            fused_embedding = self.alpha * self.dropout(visualbert_embedding) + (1-self.alpha) * self.dropout(caption_embeddings)\n",
    "            # self.wandb_run.log({\"alpha\": self.alpha.data.cpu().numpy()},commit=False)\n",
    "        \n",
    "            logits = self.cls(fused_embedding)\n",
    "        if self.fusion_method=='linear_weight_ensemble':\n",
    "            # funsion model: weight ensenble of the two embeddings: alpha*visualbert_embedding + (1-alpha)*caption_embeddings \n",
    "            fused_embedding = self.alpha * self.cls_visual(visualbert_embedding) + (1-self.alpha) * self.cls_text(caption_embeddings)\n",
    "            self.wandb_run.log({\"alpha\": self.alpha.data.cpu().numpy()},commit=False)\n",
    "        \n",
    "            logits = self.cls(fused_embedding)\n",
    "        \n",
    "        if self.fusion_method=='visualbert':\n",
    "            logits = self.cls(self.dropout(visualbert_embedding))\n",
    "            \n",
    "        if self.fusion_method=='concatenate':\n",
    "            # funsion model: concatenate the two embeddings\n",
    "            fused_embedding = torch.cat((visualbert_embedding, caption_embeddings), dim=1)\n",
    "            logits = self.cls(fused_embedding)\n",
    "        \n",
    "        \n",
    "        reshaped_logits = logits.view(-1, self.num_labels)\n",
    "        loss = self.loss_fct(reshaped_logits, labels.view(-1))\n",
    "      \n",
    "        return loss, reshaped_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b78064a-3768-4aca-9eb5-8158619b4977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchangchuntzu\u001b[0m (\u001b[33mjessica-chang\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cchang/EE559_DL/DeepLearning/wandb/run-20240526_104208-14ddblxh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jessica-chang/huggingface/runs/14ddblxh' target=\"_blank\">young-firefly-2</a></strong> to <a href='https://wandb.ai/jessica-chang/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jessica-chang/huggingface' target=\"_blank\">https://wandb.ai/jessica-chang/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jessica-chang/huggingface/runs/14ddblxh' target=\"_blank\">https://wandb.ai/jessica-chang/huggingface/runs/14ddblxh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.3082611560821533, 'eval_accuracy': 0.644, 'eval_auroc': 0.6419804451841065, 'eval_f1': 0.5679611650485437, 'eval_precision': 0.7090909090909091, 'eval_recall': 0.47368421052631576, 'eval_runtime': 38.7032, 'eval_samples_per_second': 12.919, 'eval_steps_per_second': 0.543}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Prepare directories\n",
    "data_folder_path = \"/scratch/izar/cchang/EE559/data/hateful_memes\" \n",
    "checkpoint_path = \"/scratch/izar/cchang/EE559/model-checkpoint/hatefulmemcladdifier_weight_ensemble_vit_smooth-totem-17_202405230523/checkpoint-3000\"\n",
    "query = 'query_8' \n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define paths for evaluation data\n",
    "validation_data_path = os.path.join(data_folder_path, 'query236/dev_seen_.jsonl')\n",
    "img_inpainted_dir = os.path.join(data_folder_path, 'img')\n",
    "visual_embed_model = 'vit'  \n",
    "fusion_method = 'weight_ensemble'\n",
    "seq_len = 50\n",
    "\n",
    "# Load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HateMemeClassifier(fusion_method=fusion_method, visual_embedder=visual_embed_model)\n",
    "model = model.to(device)\n",
    "\n",
    "# Load the model weights from the safetensors file\n",
    "state_dict = load_file(os.path.join(checkpoint_path, \"model.safetensors\"))\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Prepare evaluation dataset\n",
    "eval_dataset = HatefulMemesData(\n",
    "    validation_data_path, \n",
    "    img_inpainted_dir, \n",
    "    tokenizer, \n",
    "    sequence_length=seq_len, \n",
    "    query=query, \n",
    "    visual_embed_model=visual_embed_model, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Define training arguments (only evaluation relevant parameters)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",  \n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becbf6f4-68e5-405e-aafa-1e387340460a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
