{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual embeddings Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Big issue with memory --> May need to move the embedding into the __getitem__ method --> Will make code slower however"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/Bert_data/'\n",
    "train_path = data_path + 'train_df_wQuery_.jsonl'\n",
    "test_path = data_path + 'test_seen_df_wQuery_.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(train_path, lines=True)\n",
    "test_df = pd.read_json(test_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of train and test set: 8500, 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lengths of train and test set: {len(train_df)}, {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>query_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42953</td>\n",
       "      <td>img/42953.png</td>\n",
       "      <td>0</td>\n",
       "      <td>its their character not their color that matters</td>\n",
       "      <td>The image presents two distinct portraits of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23058</td>\n",
       "      <td>img/23058.png</td>\n",
       "      <td>0</td>\n",
       "      <td>don't be afraid to love again everyone is not ...</td>\n",
       "      <td>The image captures a tender moment between a m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13894</td>\n",
       "      <td>img/13894.png</td>\n",
       "      <td>0</td>\n",
       "      <td>putting bows on your pet</td>\n",
       "      <td>The image presents two photos of a gray cat ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37408</td>\n",
       "      <td>img/37408.png</td>\n",
       "      <td>0</td>\n",
       "      <td>i love everything and everybody! except for sq...</td>\n",
       "      <td>The image is a collage of two photos featuring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82403</td>\n",
       "      <td>img/82403.png</td>\n",
       "      <td>0</td>\n",
       "      <td>everybody loves chocolate chip cookies, even h...</td>\n",
       "      <td>The image is a black and white photograph feat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            img  label  \\\n",
       "0  42953  img/42953.png      0   \n",
       "1  23058  img/23058.png      0   \n",
       "2  13894  img/13894.png      0   \n",
       "3  37408  img/37408.png      0   \n",
       "4  82403  img/82403.png      0   \n",
       "\n",
       "                                                text  \\\n",
       "0   its their character not their color that matters   \n",
       "1  don't be afraid to love again everyone is not ...   \n",
       "2                           putting bows on your pet   \n",
       "3  i love everything and everybody! except for sq...   \n",
       "4  everybody loves chocolate chip cookies, even h...   \n",
       "\n",
       "                                             query_1  \n",
       "0  The image presents two distinct portraits of a...  \n",
       "1  The image captures a tender moment between a m...  \n",
       "2  The image presents two photos of a gray cat ad...  \n",
       "3  The image is a collage of two photos featuring...  \n",
       "4  The image is a black and white photograph feat...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucha\\anaconda3\\envs\\mnlp-venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importing Bert\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest description: 841\n"
     ]
    }
   ],
   "source": [
    "max_len_train = train_df['query_1'].apply(lambda n: len(n.split())).max()\n",
    "print(f\"Longest description: {max_len_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, model, max_len = 512):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = df['query_1']\n",
    "        self.labels = df['label']\n",
    "        self.ids = df['id']\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Bert Model\n",
    "        self.model = model\n",
    "        \n",
    "        self.embeddings = []\n",
    "        sentences = df['query_1'].tolist() # List of sentences\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            sentences,\n",
    "            padding=True,\n",
    "            truncation=True,  # Apply truncation to ensure sequences don't exceed the model's token limit\n",
    "            return_tensors='pt'  # Return PyTorch tensors\n",
    "        )       \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "                \n",
    "        # Get the embeddings of the [CLS] token\n",
    "        embeddings_list = outputs.last_hidden_state[:,0,:].tolist() # output is a context vector of 768 dimensions\n",
    "        self.embeddings = embeddings_list\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'id': self.ids[idx],\n",
    "            'embeddings': torch.tensor(self.embeddings[idx], dtype=torch.float),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit to 50 samples to ensure that I have enough memory\n",
    "train_dataset = TextDataset(train_df[:100], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of ids is: 100\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(f\"The number of ids is: {(len(train_dataset))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun test with Tiny neural net\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 100)\n",
    "        self.fc2 = nn.Linear(100, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model_ = TinyNet()\n",
    "\n",
    "# Losses and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_.parameters(), lr = 0.01)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 4, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataLoader is not empty.\n"
     ]
    }
   ],
   "source": [
    "def is_empty(data_loader):\n",
    "    try:\n",
    "        iterator = iter(data_loader)\n",
    "        next(iterator)\n",
    "        return False\n",
    "    except StopIteration:\n",
    "        return True\n",
    "\n",
    "# Check if train_loader is empty\n",
    "if is_empty(train_loader):\n",
    "    print(\"The DataLoader is empty.\")\n",
    "else:\n",
    "    print(\"The DataLoader is not empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(15):  # 15 epochs\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a dictionary\n",
    "        inputs, labels = data['embeddings'], data['labels']\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TextDataset(test_df[:50], tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test data: 6 %\n"
     ]
    }
   ],
   "source": [
    "# switch the model to evaluation mode\n",
    "model_.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data['embeddings'], data['labels']\n",
    "        outputs = model_(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test data: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnlp-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
