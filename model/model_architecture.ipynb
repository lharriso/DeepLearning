{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALLING PYTORCH\n",
    "#For cpu only\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml==5.1\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git@v0.5'\n",
    "!pip install pandas\n",
    "!pip install transformers\n",
    "!pip install -U scikit-learn scipy matplotlib\n",
    "!pip install datasets \n",
    "# if got error: \n",
    "# ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\n",
    "#Rememeber to restart the kernel after updagrade\n",
    "# !pip install accelerate --upgrade "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Reference: \n",
    "- https://colab.research.google.com/drive/1bLGxKdldwqnMVA5x4neY7-l_8fKGWQYI?usp=sharing#scrollTo=7-5rqN-vtlkq\n",
    "- https://github.com/Ikea-179/Hateful-Meme-Detection/blob/main/VisualBERT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from visual_embedding.visual_embeding_detectron2 import VisualEmbedder\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.structures.image_list import ImageList\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputs\n",
    "from detectron2.structures.boxes import Boxes\n",
    "from detectron2.layers import nms\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500\n",
      "dict_keys(['id', 'img', 'label', 'text', 'query_1'])\n"
     ]
    }
   ],
   "source": [
    "data_path='../data/hateful_memes/train_df_wQuery_.jsonl'\n",
    "import pandas as pd\n",
    "img_data = pd.read_json(path_or_buf=data_path, lines=True).to_dict(orient='records')\n",
    "print(len(img_data))\n",
    "print(img_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HatefulMemesData(Dataset):\n",
    "    def __init__(self, df,img_dir, tokenizer, sequence_length,caption_sequence_length=512, visual_embed_model='vit', print_text=False, visual_embeder_detecron2=None):         \n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.caption_sequence_length= caption_sequence_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_text = print_text\n",
    "        self.dataset = pd.read_json(path_or_buf=df, lines=True).to_dict(orient='records')\n",
    "        self.img_dir = img_dir\n",
    "        self.visual_embed_model = visual_embed_model\n",
    "        self.feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.feature_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k').to('cuda')\n",
    "        if self.visual_embed_model=='detectron2' and visual_embeder_detecron2 is not None:\n",
    "            self.visualembedder=visual_embeder_detecron2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "    def tokenize_data(self, example):\n",
    "   \n",
    "        idx = example['id']\n",
    "        # idx = [idx] if isinstance(idx, str) else idx\n",
    "        \n",
    "        encoded_dict = self.tokenizer(example['text'], padding='max_length', max_length=self.sequence_length,padding=True, truncation=True, return_tensors='pt')\n",
    "        tokens = encoded_dict['input_ids']\n",
    "        token_type_ids = encoded_dict['token_type_ids']\n",
    "        attn_mask = encoded_dict['attention_mask']\n",
    "        \n",
    "        captioning_encode_dict=self.tokenizer(example['query_1'], padding='max_length', max_length=self.caption_sequence_length,padding=True, truncation=True, return_tensors='pt')\n",
    "        caption_token=captioning_encode_dict['input_ids']\n",
    "        caption_token_type_ids=captioning_encode_dict['token_type_ids']\n",
    "        caption_attn_mask=captioning_encode_dict['attention_mask']\n",
    "\n",
    "        targets = torch.tensor(example['label']).type(torch.int64)\n",
    "\n",
    "        ## Get Visual Embeddings\n",
    "        try:\n",
    "            if self.visual_embed_model=='vit':\n",
    "                #TODO: make it work\n",
    "                img = example['img'].split('/')[-1]\n",
    "                img = Image.open(os.path.join(self.img_dir , img))\n",
    "                img = np.array(img)\n",
    "                img = img[...,:3]\n",
    "                inputs = self.feature_extractor(images=img, return_tensors=\"pt\")\n",
    "                outputs = self.feature_model(**inputs.to('cuda'))\n",
    "                visual_embeds = outputs.last_hidden_state\n",
    "                visual_embeds = visual_embeds.cpu() #\n",
    "            elif self.visual_embed_model=='detectron2':\n",
    "                visual_embeds = self.visualembedder.visual_embeds_detectron2([cv2.imread(os.path.join(self.img_dir, example['img'].split('/')[-1]))])[0]\n",
    "\n",
    "        except:\n",
    "            # print(\"Error with Id: \", idx)\n",
    "            if self.visual_embed_model=='vit':\n",
    "                visual_embeds = np.zeros(shape=(197, 768), dtype=float)\n",
    "            elif self.visual_embed_model=='detectron2':\n",
    "                visual_embeds = np.zeros(shape=(100, 1024), dtype=float)\n",
    "\n",
    "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "\n",
    "        inputs={\"input_ids\": tokens.squeeze(),\n",
    "            \"attention_mask\": attn_mask.squeeze(),\n",
    "            \"token_type_ids\": token_type_ids.squeeze(),\n",
    "            \"visual_embeds\": visual_embeds.squeeze(),\n",
    "            \"visual_token_type_ids\": visual_token_type_ids.squeeze(),\n",
    "            \"visual_attention_mask\": visual_attention_mask.squeeze(),\n",
    "            \"label\": targets.squeeze(),\n",
    "            \"caption_input_ids\": caption_token.squeeze(),\n",
    "            \"caption_attention_mask\": caption_attn_mask.squeeze(),\n",
    "            \"caption_token_type_ids\": caption_token_type_ids.squeeze()\n",
    "        }\n",
    "        \n",
    "        return inputs\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenize_data(self.dataset[index])\n",
    "        \n",
    "        if self.print_text:\n",
    "            for k in inputs.keys():\n",
    "                print(k, inputs[k].shape, inputs[k].dtype)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, VisualBertModel, TrainingArguments, Trainer, VisualBertConfig\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "class VisualBERTClassifier(torch.nn.Module):\n",
    "    def __init__(self, visual_embedder='vit'):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(VisualBERTClassifier, self).__init__()\n",
    "        configuration = VisualBertConfig.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre',\n",
    "                                                hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1)\n",
    "        self.visualbert = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre', config=configuration)\n",
    "        self.bertmodel = BertModel.from_pretrained('bert-base-uncased')\n",
    "        for param in self.visualbert.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.bertmodel.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if visual_embedder=='vit':\n",
    "            self.embed_cls = nn.Linear(768, 1024)\n",
    "        elif visual_embedder=='detectron2':\n",
    "            self.embed_cls = nn.Linear(1024, 1024)\n",
    "        # self.visualbert = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre')\n",
    "        self.num_labels = 2\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.cls=  nn.Linear(768, self.num_labels)\n",
    "\n",
    "        # TODO: Calculate the weights for the loss function and weight balanced loss\n",
    "        # nSamples = [5178, 2849]\n",
    "        # normedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n",
    "        # self.loss_fct = CrossEntropyLoss(weight=torch.FloatTensor(normedWeights))\n",
    "        self.loss_fct = CrossEntropyLoss()\n",
    "        \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask,\n",
    "                visual_token_type_ids, labels,caption_input_ids, caption_attention_mask, caption_token_type_ids):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        visual_embeds_cls = self.embed_cls(visual_embeds)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.visualbert(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    visual_embeds=visual_embeds_cls,\n",
    "                    visual_attention_mask=visual_attention_mask,\n",
    "                    visual_token_type_ids=visual_token_type_ids,\n",
    "                )\n",
    "        \n",
    "        visualbert_embedding = outputs[1]\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            caption_outputs = self.bert_model(caption_input_ids, attention_mask=caption_attention_mask, token_type_ids=caption_token_type_ids)\n",
    "                \n",
    "        # Get the embeddings of the [CLS] token\n",
    "        caption_embeddings = caption_outputs.last_hidden_state[:,0,:].tolist() # output is a context vector of 768 dimensions\n",
    "        \n",
    "\n",
    "        ##TODO: add fusion model here\n",
    "        logits = self.cls(#TODO)#change classification layer based on you model\n",
    "        ##\n",
    "        \n",
    "        reshaped_logits = logits.view(-1, self.num_labels)\n",
    "        loss = self.loss_fct(reshaped_logits, labels.view(-1))\n",
    "      \n",
    "        return loss, reshaped_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To check model architecture and shape for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=VisualBERTClassifier()\n",
    "for layer_name, params in model.named_parameters():\n",
    "    print(layer_name, params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from datasets import load_metric\n",
    "acc_metric = load_metric('accuracy')\n",
    "f1_metric = load_metric('f1')\n",
    "precision_metric = load_metric('precision')\n",
    "recall_metric = load_metric('recall')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)\n",
    "    auc_score = roc_auc_score(labels, predictions)\n",
    "    return {\"accuracy\": acc['accuracy'], \"auroc\": auc_score,'f1':f1['f1'],'precision':precision['precision'],'recall':recall['recall']} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define training hyperparameters and all model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, VisualBertForPreTraining, AutoTokenizer\n",
    "import time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_data_path='../data/hateful_memes/train_df_wQuery_.jsonl'\n",
    "validation_data_path='../data/hateful_memes/dev_seen_df_wQuery_.jsonl'\n",
    "img_inpainted_dir='../data/hateful_memes/img_inpainted'\n",
    "visual_embed_model='vit'\n",
    "## For visual_embed_model='detectron2'\n",
    "# cfg_path=\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
    "# MIN_BOXES=10 \n",
    "# MAX_BOXES=100\n",
    "# visualembedder_detectron2=VisualEmbedder(cfg_path=cfg_path, min_boxes=MIN_BOXES, max_boxes=MAX_BOXES)\n",
    "##\n",
    "output_dir=os.path.join('model-checkpoint', f'visualbert_{visual_embed_model}_{time.strftime(\"%Y%m%d%H%M\")}')\n",
    "\n",
    "batch_size = 24\n",
    "seq_len = 50\n",
    "model = VisualBERTClassifier()\n",
    "model = model.cuda()\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    seed = 110, \n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs= 3,\n",
    "    weight_decay=0.05,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"auroc\",\n",
    "    eval_steps = 250,\n",
    "    save_steps = 500,\n",
    "    fp16 = False,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = HatefulMemesData(train_data_path, img_inpainted_dir, tokenizer, sequence_length=seq_len, visual_embed_model=visual_embed_model),\n",
    "    eval_dataset =  HatefulMemesData(validation_data_path, img_inpainted_dir,tokenizer, sequence_length=seq_len, visual_embed_model=visual_embed_model),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train + evaluation on validation state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1065' max='1065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1065/1065 23:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auroc</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.721819</td>\n",
       "      <td>0.506000</td>\n",
       "      <td>0.500144</td>\n",
       "      <td>0.023715</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.012146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.731503</td>\n",
       "      <td>0.508000</td>\n",
       "      <td>0.502072</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.008097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>0.735651</td>\n",
       "      <td>0.508000</td>\n",
       "      <td>0.502024</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.661600</td>\n",
       "      <td>0.737463</td>\n",
       "      <td>0.508000</td>\n",
       "      <td>0.502024</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.004049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1065, training_loss=0.6695632540564023, metrics={'train_runtime': 1384.2411, 'train_samples_per_second': 18.422, 'train_steps_per_second': 0.769, 'total_flos': 0.0, 'train_loss': 0.6695632540564023, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Inference and evaluate on test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
