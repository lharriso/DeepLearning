{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTALLING PYTORCH\n",
    "#For cpu only\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml==5.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (5.1)\n",
      "Collecting git+https://github.com/facebookresearch/detectron2.git@v0.5\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git (to revision v0.5) to /tmp/pip-req-build-n089rgpd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-n089rgpd\n",
      "  Running command git checkout -q 82a57ce0b70057685962b352535147d9a8118578\n",
      "  Resolved https://github.com/facebookresearch/detectron2.git to commit 82a57ce0b70057685962b352535147d9a8118578\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=7.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (9.5.0)\n",
      "Requirement already satisfied: matplotlib in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (3.8.4)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (2.0.7)\n",
      "Requirement already satisfied: termcolor>=1.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (2.4.0)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (0.1.8)\n",
      "Requirement already satisfied: tabulate in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (3.0.0)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (4.66.2)\n",
      "Requirement already satisfied: tensorboard in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (2.16.2)\n",
      "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (0.1.9)\n",
      "Requirement already satisfied: future in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (1.0.0)\n",
      "Requirement already satisfied: pydot in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (2.0.0)\n",
      "Requirement already satisfied: omegaconf>=2.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (2.3.0)\n",
      "Requirement already satisfied: hydra-core>=1.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (1.3.2)\n",
      "Requirement already satisfied: black==21.4b2 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from detectron2==0.5) (21.4b2)\n",
      "Requirement already satisfied: click>=7.1.2 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from black==21.4b2->detectron2==0.5) (8.1.7)\n",
      "Requirement already satisfied: appdirs in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from black==21.4b2->detectron2==0.5) (1.4.4)\n",
      "Requirement already satisfied: toml>=0.10.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from black==21.4b2->detectron2==0.5) (0.10.2)\n",
      "Requirement already satisfied: regex>=2020.1.8 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from black==21.4b2->detectron2==0.5) (2024.4.28)\n",
      "Requirement already satisfied: pathspec<1,>=0.8.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from black==21.4b2->detectron2==0.5) (0.12.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from black==21.4b2->detectron2==0.5) (1.0.0)\n",
      "Requirement already satisfied: numpy in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.5) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.5) (5.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from hydra-core>=1.1->detectron2==0.5) (4.9.3)\n",
      "Requirement already satisfied: packaging in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from hydra-core>=1.1->detectron2==0.5) (24.0)\n",
      "Requirement already satisfied: portalocker in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.5) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib->detectron2==0.5) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib->detectron2==0.5) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib->detectron2==0.5) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib->detectron2==0.5) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib->detectron2==0.5) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib->detectron2==0.5) (2.9.0.post0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (1.62.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (5.26.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.5) (2.1.3)\n",
      "Requirement already satisfied: pandas in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: transformers in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (4.40.1)\n",
      "Requirement already satisfied: filelock in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from transformers) (5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from transformers) (2024.4.28)\n",
      "Requirement already satisfied: requests in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: scikit-learn in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (1.4.2)\n",
      "Requirement already satisfied: scipy in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (1.13.0)\n",
      "Requirement already satisfied: matplotlib in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (3.8.4)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from datasets) (4.66.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from datasets) (5.1)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Using cached datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-16.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m93.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, pyarrow-hotfix, pyarrow, multidict, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.19.1 dill-0.3.8 frozenlist-1.4.1 multidict-6.0.5 multiprocess-0.70.16 pyarrow-16.0.0 pyarrow-hotfix-0.6 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml==5.1\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git@v0.5'\n",
    "!pip install pandas\n",
    "!pip install transformers\n",
    "!pip install -U scikit-learn scipy matplotlib\n",
    "!pip install datasets\n",
    "!pip install accelerate -U\n",
    "!pip install scikit-learn\n",
    "!pip install wandb\n",
    "# if got error: \n",
    "# ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\n",
    "#Rememeber to restart the kernel after updagrade\n",
    "# !pip install accelerate --upgrade "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Reference: \n",
    "- https://colab.research.google.com/drive/1bLGxKdldwqnMVA5x4neY7-l_8fKGWQYI?usp=sharing#scrollTo=7-5rqN-vtlkq\n",
    "- https://github.com/Ikea-179/Hateful-Meme-Detection/blob/main/VisualBERT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import os\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, VisualBertForPreTraining, AutoTokenizer\n",
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500\n",
      "dict_keys(['id', 'img', 'label', 'text', 'query_1'])\n"
     ]
    }
   ],
   "source": [
    "data_path='/scratch/izar/cchang/EE559/data/hateful_memes/train_df_wQuery_.jsonl'\n",
    "import pandas as pd\n",
    "img_data = pd.read_json(path_or_buf=data_path, lines=True).to_dict(orient='records')\n",
    "print(len(img_data))\n",
    "print(img_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGzCAYAAAA1yP25AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABKlklEQVR4nO3deXxM9/4/8NdM9j0R2RASooIEtSekFVusV4q2ailKKaHE9m20FVylqNLW7qqUUr08UlpqTS0p0UtcS2yNEEuzWLMI2WY+vz/6m3MdCTJMMjmT1/PxmEdzPuczZ96ZaOaVz/mcz1EJIQSIiIiIFEht7AKIiIiIXhSDDBERESkWgwwREREpFoMMERERKRaDDBERESkWgwwREREpFoMMERERKRaDDBERESkWgwwREREpFoMMkZGoVCrMnDnT2GXIHD9+HMHBwbCzs4NKpcKpU6eMXRKZEB8fH/Tq1cvYZZCJYZAhkxMTEwOVSiV7uLu7IzQ0FLt27TJ2eS/t/PnzmDlzJlJTUw163KKiIrz55pu4d+8eFi9ejA0bNqBOnTrPfM7169fxwQcfwMfHB1ZWVnB3d8cbb7yBo0ePGrS2ipKamlri387THoZ6//X9ec6cORMqlQp37twxyOsbWnn9+yR6GnNjF0BUXmbPng1fX18IIZCZmYmYmBj06NEDv/zyi6L/Kjx//jxmzZqFDh06wMfHx2DHTUlJwbVr17BmzRqMHDnyuf2PHDmCHj16AABGjhyJRo0aISMjAzExMWjfvj2WLVuGMWPGGKy+iuDm5oYNGzbI2hYtWoSbN29i8eLFJfoaQnn9PI3F1L4fqvwYZMhkde/eHS1btpS2R4wYAQ8PD/zwww+KDjLl5datWwAAZ2fn5/a9f/8++vfvDxsbGxw5cgT16tWT9k2aNAlhYWEYP348Xn31VbRt27a8Si5VXl4e7OzsXui5dnZ2GDx4sKxt8+bNuH//fol2IqoceGqJqgxnZ2fY2NjA3Fye3/Py8jB58mR4e3vDysoKDRo0wBdffAHdjeEfPXoEf39/+Pv749GjR9Lz7t27By8vLwQHB0Oj0QAAhg0bBnt7e1y5cgVhYWGws7NDjRo1MHv2bJTlRvP//e9/0b17dzg6OsLe3h6dOnXCsWPHpP0xMTF48803AQChoaHSaY6DBw8+87i//fYbQkJCYGdnB2dnZ/Tp0wcXLlyQ9g8bNgyvv/46AODNN9+ESqVChw4dnnq8VatWISMjAwsXLpSFGACwsbHBd999B+DvUTEd3SmRJ+lOBT55KmLXrl1SzQ4ODujZsyfOnTsn66N7v1NSUtCjRw84ODhg0KBBiI6OhoWFBW7fvl3i9UaNGgVnZ2fk5+c/9ft7noKCAkRHR8PPzw9WVlbw9vbGtGnTUFBQIPUZOnQorK2tZe8zAISFhcHFxQVpaWkv/PMsi4sXL6J///6oVq0arK2t0bJlS/z888+yPrr3/siRI5g0aRLc3NxgZ2eHN954o8R7p9VqMXPmTNSoUQO2trYIDQ3F+fPn4ePjg2HDhknHK8v38/vvv6N169awtrZG3bp1sX79etn+oqIizJo1C/Xr14e1tTVcXV3Rvn177Nu376XfFzJBgsjErFu3TgAQ+/fvF7dv3xa3bt0SSUlJYvTo0UKtVou9e/dKfbVarejYsaNQqVRi5MiRYunSpaJ3794CgJg4caLU79ixY8LMzExERkZKbQMGDBA2Njbi0qVLUtvQoUOFtbW1qF+/vhgyZIhYunSp6NWrlwAgPv30U1mdAER0dLS0nZSUJOzs7ISXl5f45z//KT7//HPh6+srrKysxLFjx4QQQqSkpIgPP/xQABDTp08XGzZsEBs2bBAZGRlPfT/27dsnzM3NxSuvvCIWLFggZs2aJapXry5cXFzE1atXhRBCHD16VEyfPl0AEB9++KHYsGGD7H16UnBwsLC2thb5+flP7fP6668LCwsL8ejRIyGEENHR0aK0Xzm6n5euFiGEWL9+vVCpVKJbt27im2++EfPnzxc+Pj7C2dlZ1m/o0KHCyspK1KtXTwwdOlSsXLlSrF+/XiQnJwsA4ptvvpG9VkFBgXBxcRHvvffeU+t+Us+ePUWdOnWkbY1GI7p27SpsbW3FxIkTxapVq8S4ceOEubm56NOnj9Tv/v37olatWqJVq1aiuLhYCCHEypUrBQCxYcMGIcSL/Tx17+Pt27ef2icpKUk4OTmJRo0aifnz54ulS5eK1157TahUKhEbGyv10733r776qujYsaP45ptvxOTJk4WZmZl46623ZMecNm2aACB69+4tli5dKt5//31Rq1YtUb16dTF06NAyfT916tQRDRo0EB4eHmL69Oli6dKlonnz5kKlUomkpCTptaZPny5UKpV4//33xZo1a8SiRYvEO++8Iz7//PNn/7CoSmKQIZOj++X85MPKykrExMTI+m7btk0AEHPmzJG19+/fX6hUKnH58mWpLSoqSqjVanH48GGxZcsWAUAsWbJE9ryhQ4cKAGL8+PFSm1arFT179hSWlpayD58ng0x4eLiwtLQUKSkpUltaWppwcHAQr732mtSme+0DBw6U6f1o1qyZcHd3F3fv3pXaTp8+LdRqtXj33XeltgMHDggAYsuWLc89prOzs2jatOkz++g+0M6cOSOEKHuQyc3NFc7OzuL999+X9cvIyBBOTk6ydt37/dFHH5U4blBQkGjTpo2sLTY2Vq/3ToiSQWbDhg1CrVaL+Ph4WT9dSDly5IjUtmfPHunf15UrV4S9vb0IDw+XPU/fn2dZgkynTp1EYGCgLGhqtVoRHBws6tevL7Xp3vvOnTsLrVYrtUdGRgozMzORlZUlhPj7vTc3Ny9R+8yZMwUAKcg87/upU6eOACAOHz4std26dUtYWVmJyZMnS21NmzYVPXv2fP6bQSSE4KklMlnLli3Dvn37sG/fPnz//fcIDQ3FyJEjERsbK/X59ddfYWZmhg8//FD23MmTJ0MIIbvKaebMmWjcuDGGDh2KsWPH4vXXXy/xPJ1x48ZJX6tUKowbNw6FhYXYv39/qf01Gg327t2L8PBw1K1bV2r38vLCwIED8fvvvyMnJ0fv9yA9PR2nTp3CsGHDUK1aNam9SZMm6NKlC3799Ve9jwkAubm5cHBweGYf3f7c3Fy9jr1v3z5kZWXhnXfewZ07d6SHmZkZ2rRpgwMHDpR4TmmTit9991388ccfSElJkdo2btwIb29v6TTai9iyZQsaNmwIf39/WX0dO3YEAFl9Xbt2xejRozF79mz07dsX1tbWWLVq1Qu/dlncu3cPv/32G9566y3k5uZK9d29exdhYWFITk7GX3/9JXvOqFGjZKf9QkJCoNFocO3aNQBAXFwciouLMXbsWNnzxo8fr3d9jRo1QkhIiLTt5uaGBg0a4MqVK1Kbs7Mzzp07h+TkZL2PT1UPgwyZrNatW6Nz587o3LkzBg0ahJ07d6JRo0ZSqACAa9euoUaNGiU+lBs2bCjt17G0tMS3336Lq1evIjc3F+vWrSt1zodarZaFEQB45ZVXAOCpl6Tevn0bDx8+RIMGDUrsa9iwIbRaLW7cuFH2b/7/09X/tOPeuXMHeXl5eh/XwcHhuQFFt9/d3V2vY+s+vDp27Ag3NzfZY+/evdKkZB1zc3PUqlWrxHHefvttWFlZYePGjQCA7Oxs7NixA4MGDSr156ZPfefOnStRm+5n/GR9X3zxBapVq4ZTp07h66+/1vv90Nfly5chhMCnn35aosbo6OhSa6xdu7Zs28XFBcDfk7qB//078vPzk/WrVq2a1Lesnnwt3evpXgv4e25VVlYWXnnlFQQGBmLq1Kk4c+aMXq9DVQevWqIqQ61WIzQ0FF999RWSk5PRuHFjvY+xZ88eAEB+fj6Sk5Ph6+tr6DIVoVGjRjh58iQKCgpgZWVVap8zZ87A0tISNWvWBICnhgfdRGkdrVYLANiwYQM8PT1L9H9ysraVlRXU6pJ/k7m4uKBXr17YuHEjZsyYga1bt6KgoOClrz7SarUIDAzEl19+Wep+b29v2fZ///tfKTicPXsW77zzzku9flnqA4ApU6YgLCys1D5PBhIzM7NS+4kyTFDXV1le67XXXkNKSgq2b9+OvXv34l//+hcWL16MlStXlmlpAKpaGGSoSikuLgYAPHjwAABQp04d7N+/v8SpkosXL0r7dc6cOYPZs2dj+PDhOHXqFEaOHImzZ8/CyclJ9hparRZXrlyR/kIHgD///BMAnrquhpubG2xtbXHp0qUS+y5evAi1Wi19QOozmqCr/2nHrV69+gtdqty7d28cPXoUW7ZsKTUYpKamIj4+Hn369IGNjQ2A//2Vn5WVJbvE+/FRLwDSVVDu7u7o3Lmz3rU97t1330WfPn1w/PhxbNy4Ea+++uoLBdgn6zt9+jQ6der03J9FXl4ehg8fjkaNGiE4OBgLFizAG2+8gVatWkl9XmZ0qDS60UALC4uXfv90dP+OLl++LAvvd+/elY2kAIb7fqpVq4bhw4dj+PDhePDgAV577TXMnDmTQYZK4KklqjKKioqwd+9eWFpaSqeOevToAY1Gg6VLl8r6Ll68GCqVCt27d5eeO2zYMNSoUQNfffUVYmJikJmZicjIyFJf6/HjCSGwdOlSWFhYoFOnTqX2NzMzQ9euXbF9+3bZ6afMzExs2rQJ7du3h6OjIwBIwSMrK+u537OXlxeaNWuG7777TtY/KSkJe/fulRa009fo0aPh6emJqVOnyuY2AH+PVg0fPhwqlQrTpk2T2nUB5fDhw1JbXl6edKm2TlhYGBwdHTF37lwUFRWVeO3SLql+mu7du6N69eqYP38+Dh06ZJC1YN566y389ddfWLNmTYl9jx49kp2q+7//+z9cv34d3333Hb788kv4+Phg6NChssu09fl5loW7uzs6dOiAVatWIT09vcR+fd4/nU6dOsHc3BwrVqyQtT/5/w1gmO/n7t27sm17e3v4+fnJ3jciHY7IkMnatWuXNLJy69YtbNq0CcnJyfjoo4+kUNC7d2+Ehobi448/RmpqKpo2bYq9e/di+/btmDhxovThO2fOHJw6dQpxcXFwcHBAkyZNMGPGDHzyySfo37+/LBBYW1tj9+7dGDp0KNq0aYNdu3Zh586dmD59+jNXg50zZw727duH9u3bY+zYsTA3N8eqVatQUFCABQsWSP2aNWsGMzMzzJ8/H9nZ2bCyskLHjh2fOvdi4cKF6N69O4KCgjBixAg8evQI33zzDZycnF74Xk8uLi7YunUrevTogebNm5dY2ffKlStYunQp2rRpIz2na9euqF27NkaMGIGpU6fCzMwM3377Ldzc3HD9+nWpn6OjI1asWIEhQ4agefPmGDBggNRn586daNeuXakfoKWxsLDAgAEDsHTpUpiZmRnktM6QIUPw73//Gx988AEOHDiAdu3aQaPR4OLFi/j3v/+NPXv2oGXLlvjtt9+wfPlyREdHo3nz5gCAdevWoUOHDvj000+ln6m+P0+dL7/8Era2trI2tVqN6dOnY9myZWjfvj0CAwPx/vvvo27dusjMzERCQgJu3ryJ06dP6/U9e3h4YMKECVi0aBH+8Y9/oFu3bjh9+jR27dqF6tWry0ZhXvT7eVyjRo3QoUMHtGjRAtWqVcOJEyewdetW2SR6IolRr5kiKgelXX5tbW0tmjVrJlasWCG7zFSIvy/3jYyMFDVq1BAWFhaifv36YuHChVK/xMREYW5uLrukWgghiouLRatWrUSNGjXE/fv3hRB/Xw5sZ2cnUlJSpLVGPDw8RHR0tNBoNLLn44nLr4UQ4uTJkyIsLEzY29sLW1tbERoaKo4ePVrie1yzZo2oW7euMDMzK9Olu/v37xft2rUTNjY2wtHRUfTu3VucP39e1kefy691UlNTxahRo0Tt2rWFubm59H7v37+/1P6JiYmiTZs2wtLSUtSuXVt8+eWXpa4jo6snLCxMODk5CWtra1GvXj0xbNgwceLECamP7v1+lv/85z8CgOjatWuZv6/HPXn5tRBCFBYWivnz54vGjRsLKysr4eLiIlq0aCFmzZolsrOzRU5OjqhTp45o3ry5KCoqkj03MjJSqNVqkZCQILXp8/PUXX5d2sPMzEzql5KSIt59913h6ekpLCwsRM2aNUWvXr3E1q1bpT669/748eOy19D9W3i8juLiYvHpp58KT09PYWNjIzp27CguXLggXF1dxQcffCB7/tO+nzp16pR6WfXrr78uXn/9dWl7zpw5onXr1sLZ2VnY2NgIf39/8dlnn4nCwsKnvi9UdamEKIfZXERV1LBhw7B161ZpDk5VExcXhx49eqB9+/bYtWsXLC0tjV0STp8+jWbNmmH9+vUYMmSIscsxKVlZWXBxccGcOXPw8ccfG7scqqI4R4aIDKZTp0747rvvcODAAQwfPrxcrnrR15o1a2Bvb4++ffsauxRFe/z2HDpLliwBgGfezoKovHGODBEZ1IABAzBgwABjl4FffvkF58+fx+rVqzFu3LgXvpEk/e3HH3+U7iBvb2+P33//HT/88AO6du2Kdu3aGbs8qsIYZIjIJI0fPx6ZmZno0aMHZs2aZexyFK9JkyYwNzfHggULkJOTI00AnjNnjrFLoyqOc2SIiIhIsThHhoiIiBSLQYaIiIgUy+TnyGi1WqSlpcHBwcHgS4ETERFR+RBCIDc3FzVq1Cj1fmo6Jh9k0tLSStzEjYiIiJThxo0bpd7hXsfkg4zuRoA3btyQlqUnIiKiyi0nJwfe3t6yG/qWxuSDjO50kqOjI4MMERGRwjxvWggn+xIREZFiMcgQERGRYjHIEBERkWIxyBAREZFiMcgQERGRYjHIEBERkWIxyBAREZFiMcgQERGRYpn8gnhEZJo0Gg3i4+ORnp4OLy8vhISEwMzMzNhlEVEF44gMESlObGws/Pz8EBoaioEDByI0NBR+fn6IjY01dmlEVMGMGmR8fHygUqlKPCIiIgAA+fn5iIiIgKurK+zt7dGvXz9kZmYas2QiMrLY2Fj0798fgYGBSEhIQG5uLhISEhAYGIj+/fszzBBVMSohhDDWi9++fRsajUbaTkpKQpcuXXDgwAF06NABY8aMwc6dOxETEwMnJyeMGzcOarUaR44cKfNr5OTkwMnJCdnZ2bzXEpHCaTQa+Pn5ITAwENu2bYNa/b+/xbRaLcLDw5GUlITk5GSeZiJSuLJ+fhs1yDxp4sSJ2LFjB5KTk5GTkwM3Nzds2rQJ/fv3BwBcvHgRDRs2REJCAtq2bVvqMQoKClBQUCBt6+6eySBDpHwHDx5EaGjoU38HJCQkIDg4WPpjiIiUq6xBptLMkSksLMT333+P9957DyqVComJiSgqKkLnzp2lPv7+/qhduzYSEhKeepx58+bByclJenh7e1dE+URUAdLT0wEAAQEB0Gg0OHjwIH744QccPHgQGo0GAQEBsn5EZPoqzVVL27ZtQ1ZWFoYNGwYAyMjIgKWlJZydnWX9PDw8kJGR8dTjREVFYdKkSdK2bkSGiJTPy8sLALB06VKsWrUKqamp0j4fHx+MGjVK1o+ITF+lGZFZu3Ytunfvjho1arzUcaysrODo6Ch7EJFpCAkJgbu7O6KiohAQECCb7BsQEIDp06fD3d0dISEhxi6ViCpIpRiRuXbtGvbv3y+72sDT0xOFhYXIysqSjcpkZmbC09PTCFUSUWXw+LQ+IYT0IKKqqVKMyKxbtw7u7u7o2bOn1NaiRQtYWFggLi5Oart06RKuX7+OoKAgY5RJREYWHx+P27dvY968eUhKSkJwcDAcHR0RHByMc+fOYe7cubh16xbi4+ONXSoRVRCjj8hotVqsW7cOQ4cOhbn5/8pxcnLCiBEjMGnSJFSrVg2Ojo4YP348goKCnnrFEhGZNt0k3nHjxmHSpElYvnw5UlJSUK9ePYwdOxYFBQWYPn06J/sSVSFGDzL79+/H9evX8d5775XYt3jxYqjVavTr1w8FBQUICwvD8uXLjVAlEVUGz5rs+9VXX3GyL1EVVKnWkSkPXBCPyHRoNBp4eXnh9u3b6NWrFz7++GMEBAQgKSkJn332GXbs2AF3d3ekpaVxQTwihVPcOjJERGWhUqmkrznZl4gYZIhIMeLj43Hr1i1O9iUiCYMMESnG45N9L1++jAMHDmDTpk04cOAAkpOTMW7cOFk/IjJ9Rp/sS0RUVrpJvElJSWjbtm2J+yklJSXJ+hGR6eOIDBEpRkhICHx8fDB37lxotVrZPq1Wi3nz5sHX15cr+xJVIQwyRKQYZmZmWLRoEXbs2IHw8HDZLQrCw8OxY8cOfPHFF7xiiagK4aklIlKUvn37YuvWrZg8eTKCg4Oldl9fX2zduhV9+/Y1YnVEVNG4jgwRKVJhYWGJlX0tLS2NXRYRGUhZP785IkNEihMbG4vJkyeXWNl30aJFHJEhqmI4R4aIFCU2Nhb9+/dHYGCgbI5MYGAg+vfvj9jYWGOXSEQViKeWiEgxNBoN/Pz8EBgYiG3btkGt/t/fYlqtFuHh4UhKSkJycjIn/BIpHG9RQEQmJz4+HqmpqZg+fbosxACAWq1GVFQUrl69ypV9iaoQBhkiUgzdir0BAQGl7te1c2VfoqqDQYaIFOPxlX1Lw5V9iaoeBhkiUgyu7EtET2KQISLF4Mq+RPQkriNDRIrClX2J6HG8/JqIFIkr+xKZNq7sS0Qmiyv7EpEO58gQkaJwZV8iehxPLRGRYnBlX6Kqgyv7EpHJ4cq+RPQkBhkiUgyu7EtET2KQISLF4Mq+RPQkBhkiUgyu7EtET2KQISLF4Mq+RPQkriNDRIrClX2J6HG8/JqIFEmj0SA+Ph7p6enw8vJCSEgIR2KITAhX9iUik2ZmZoYOHToYuwwiMjLOkSEiIiLFYpAhIiIixWKQISIiIsVikCEiIiLFYpAhIiIixWKQISIiIsVikCEiIiLF4joyRKRIXBCPiIBKMCLz119/YfDgwXB1dYWNjQ0CAwNx4sQJab8QAjNmzICXlxdsbGzQuXNnJCcnG7FiIjK22NhY+Pn5ITQ0FAMHDkRoaCj8/PwQGxtr7NKIqIIZNcjcv38f7dq1g4WFBXbt2oXz589j0aJFcHFxkfosWLAAX3/9NVauXIk//vgDdnZ2CAsLQ35+vhErJyJjiY2NRf/+/REYGCi7aWRgYCD69+/PMENUxRj1XksfffQRjhw5gvj4+FL3CyFQo0YNTJ48GVOmTAEAZGdnw8PDAzExMRgwYMBzX4P3WiIyHRqNBn5+fggMDMS2bdugVv/vbzGtVovw8HAkJSUhOTmZp5mIFK6sn99GHZH5+eef0bJlS7z55ptwd3fHq6++ijVr1kj7r169ioyMDHTu3Flqc3JyQps2bZCQkFDqMQsKCpCTkyN7EJFpiI+PR2pqKqZPny4LMQCgVqsRFRWFq1evPvWPIyIyPUYNMleuXMGKFStQv3597NmzB2PGjMGHH36I7777DgCQkZEBAPDw8JA9z8PDQ9r3pHnz5sHJyUl6eHt7l+83QUQVJj09HQAQEBCAwsJCLFmyBOPHj8eSJUtQWFiIgIAAWT8iMn1GvWpJq9WiZcuWmDt3LgDg1VdfRVJSElauXImhQ4e+0DGjoqIwadIkaTsnJ4dhhshEeHl5AQA++OAD/PjjjyguLpb2TZ06FW+99ZasHxGZPqOOyHh5eaFRo0aytoYNG+L69esAAE9PTwBAZmamrE9mZqa070lWVlZwdHSUPYjINISEhMDJyQkbN25EtWrVsGbNGqSnp2PNmjWoVq0aNm3aBCcnJ4SEhBi7VCKqIEYNMu3atcOlS5dkbX/++Sfq1KkDAPD19YWnpyfi4uKk/Tk5Ofjjjz8QFBRUobUSkfFpNBrk5uYCAFq3bo3GjRvDzs4OjRs3RuvWrQEAubm50Gg0xiyTiCqQUYNMZGQkjh07hrlz5+Ly5cvYtGkTVq9ejYiICACASqXCxIkTMWfOHPz88884e/Ys3n33XdSoUQPh4eHGLJ2IjGD58uXQarUYM2YMkpKSEBwcDEdHRwQHB+PcuXMYPXo0tFotli9fbuxSiaiCGHWOTKtWrfDTTz8hKioKs2fPhq+vL5YsWYJBgwZJfaZNm4a8vDyMGjUKWVlZaN++PXbv3g1ra2sjVk5ExpCSkgIAmDFjBpYsWYLly5cjJSUF9erVw9ixY3Hnzh2sWrVK6kdEps/otyjo1asXevXq9dT9KpUKs2fPxuzZsyuwKiKqjOrVqwcAmD17Nnbt2oXU1FRp31dffYVu3brJ+hGR6TPqgngVgQviEZmOwsJC2NjYQKvVomfPnvjkk08QEBCApKQkzJkzBzt37oRarcajR49gaWlp7HKJ6CUoYkE8IiJ9mJmZwd7eHgBw4sQJnDlzBjk5OThz5ox0jzZ7e3uu6ktUhTDIEJFixMfHIycnB4MGDcLdu3cxevRo1KxZE6NHj8bdu3cxcOBA5OTkcGVfoiqEQYaIFEO3Yu/KlSuRk5ODiIgIdO3aFREREcjJycHKlStl/YjI9Bl9si8RUVnpVuxdunQpVq1aJU323bt3L3bu3IlRo0bJ+hGR6eOIDBEpRkhICNzc3BAVFYWAgAAkJCQgNzcXCQkJCAgIwPTp0+Hu7s6VfYmqEAYZIlIUlUolfS2EkB5EVDUxyBCRYsTHx+PWrVuYN29eqSv7zp07F7du3eJkX6IqhEGGiBRDN4l33LhxuHz5Mg4cOIBNmzbhwIEDSE5Oxrhx42T9iMj0cbIvESmGbhJvUlIS2rZtiw4dOsj2JyUlyfoRkenjiAwRKUZISAh8fHwwd+5caLVa2T6tVot58+bB19eXk32JqhAGGSJSDDMzMyxatAg7duxAeHi47Kql8PBw7NixA1988QVX9iWqQnhqiYgUpW/fvti6dSsmT56M4OBgqd3X1xdbt25F3759jVgdEVU03jSSiBRJo9EgPj4e6enp8PLyQkhICEdiiExIWT+/OSJDRIpkZmZWYrIvEVU9nCNDREREisUgQ0RERIrFIENERESKxSBDREREisUgQ0RERIrFIENERESKxSBDREREisV1ZIhIkbggHhEBHJEhIgWKjY2Fn58fQkNDMXDgQISGhsLPzw+xsbHGLo2IKhiDDBEpSmxsLPr374/AwEDZTSMDAwPRv39/hhmiKob3WiIixdBoNPDz80NgYCC2bdsGtfp/f4tptVqEh4cjKSkJycnJPM1EpHBl/fzmiAwRKUZ8fDxSU1Mxffp0WYgBALVajaioKFy9ehXx8fFGqpCIKhqDDBEpRnp6OgAgICCg1P26dl0/IjJ9DDJEpBheXl4AgKSkpFL369p1/YjI9DHIEJFihISEwMfHB3PnzoVWq5Xt02q1mDdvHnx9fRESEmKkComoojHIEJFimJmZYdGiRdixYwfCw8NlVy2Fh4djx44d+OKLLzjRl6gK4YJ4RKQoffv2xdatWzF58mQEBwdL7b6+vti6dSv69u1rxOqIqKLx8msiUiSu7Etk2sr6+c0RGSJSJDMzM3To0MHYZRCRkXGODBERESkWgwwREREpFoMMERERKRbnyBCRIhUWFmL58uVISUlBvXr1MHbsWFhaWhq7LCKqYEYdkZk5cyZUKpXs4e/vL+3Pz89HREQEXF1dYW9vj379+iEzM9OIFRNRZTBt2jTY2dkhMjISS5cuRWRkJOzs7DBt2jRjl0ZEFczop5YaN26M9PR06fH7779L+yIjI/HLL79gy5YtOHToENLS0rhGBFEVN23aNCxcuBCurq5Ys2YN0tPTsWbNGri6umLhwoUMM0RVjFHXkZk5cya2bduGU6dOldiXnZ0NNzc3bNq0Cf379wcAXLx4EQ0bNkRCQgLatm1bptfgOjJEpqOwsBB2dnZwdXXFzZs3YW7+v7PjxcXFqFWrFu7evYu8vDyeZiJSuLJ+fht9RCY5ORk1atRA3bp1MWjQIFy/fh0AkJiYiKKiInTu3Fnq6+/vj9q1ayMhIeGpxysoKEBOTo7sQUSmYfny5SguLsacOXNkIQYAzM3NMXv2bBQXF2P58uVGqpCIKppRg0ybNm0QExOD3bt3Y8WKFbh69SpCQkKQm5uLjIwMWFpawtnZWfYcDw8PZGRkPPWY8+bNg5OTk/Tw9vYu5++CiCpKSkoKAKBXr16l7te16/oRkekzapDp3r073nzzTTRp0gRhYWH49ddfkZWVhX//+98vfMyoqChkZ2dLjxs3bhiwYiIypnr16gEAduzYUep+XbuuHxGZPqOfWnqcs7MzXnnlFVy+fBmenp4oLCxEVlaWrE9mZiY8PT2fegwrKys4OjrKHkRkGsaOHQtzc3N88sknKC4ulu0rLi7GjBkzYG5ujrFjxxqpQiKqaJUqyDx48AApKSnw8vJCixYtYGFhgbi4OGn/pUuXcP36dQQFBRmxSiIyFktLS0RGRiIzMxO1atXC6tWrkZaWhtWrV6NWrVrIzMxEZGQkJ/oSVSFGXRBvypQp6N27N+rUqYO0tDRER0fDzMwM77zzDpycnDBixAhMmjQJ1apVg6OjI8aPH4+goKAyX7FERKZnwYIFAIDFixdj9OjRUru5uTmmTp0q7SeiqsGoQebmzZt45513cPfuXbi5uaF9+/Y4duwY3NzcAPz9i0qtVqNfv34oKChAWFgYr0YgIixYsAAzZszAkCFDpJV9N2zYAHt7e2OXRkQVzKjryFQEriNDZHqmTZuGxYsXy+bJmJubIzIykiMyRCZCMevIEBHpgyv7EtHjOCJDRIrBlX2Jqg6OyBCRyXl8ZV+tVoslS5Zg/PjxWLJkCbRaLVf2JaqC9J7sq9FoEBMTg7i4ONy6dQtarVa2/7fffjNYcUREj9Ot2Hvy5EmMGTNGNkdm6tSpeP/992X9iMj06T0iM2HCBEyYMAEajQYBAQFo2rSp7EFEVF50K/auWLGi1DkyK1askPUjItOn9xyZ6tWrY/369ejRo0d51WRQnCNDZDoePHgABwcHqFQqPHz4ENbW1tK+/Px82NraQgiB3NxcXopNpHDlNkfG0tISfn5+L1UcEdGL+Ne//gUAEELAx8dHtrKvj48PdH+X6foRkenTO8hMnjwZX331FUz8YiciqoR0c1/GjBmDO3fuYPTo0ahZsyZGjx6NO3fuYMyYMbJ+RGT6yjTZt2/fvrLt3377Dbt27ULjxo1hYWEh2xcbG2u46oiIHqOb+3LlypVS9+sCDOfIEFUdZZojM3z48DIfcN26dS9VkKFxjgyR6SgsLIS1tTWEEHB3d8dnn32GXr16YceOHfj4449x69YtqFQq5Ofncx0ZIoUr6+d3mUZkKls4ISICAK1WKz2IqGrSe45Mx44dkZWVVaI9JycHHTt2NERNRESlWr58OYQQCAsLw71792RzZO7du4cuXbpACMEF8YiqEL0XxDt48CAKCwtLtOfn5yM+Pt4gRRERlUY3ByYmJgb29vYl7n6dk5ODmjVrcrIvURVS5iBz5swZ6evz588jIyND2tZoNNi9ezdq1qxp2OqIiB6jm8Q7bNgwxMXFSSv7nj17Fi4uLggNDZX1IyLTV+YF8dRqNVQqFQCUeum1jY0NvvnmG7z33nuGrfAlcbIvkengZF+iqsOgk30B4OrVqxBCoG7duvjPf/4DNzc3aZ+lpSXc3d1hZmb2clUTEemBk32JqMxBpk6dOgDAXxhEZDSPT/aNi4vD6NGjpX3m5ubo2rUr9u7di+XLl2PixInGK5SIKozek31//vnnUttVKhWsra3h5+cHX1/fly6MiOhJj0/2NTMzQ+vWrXH79m24ubnhP//5D4qKijjZl6iK0TvIhIeHQ6VSlZgno2tTqVRo3749tm3bBhcXF4MVSkSkm8TbpEkT3L59W2rPy8uDu7u7dMqbk32Jqg6915HZt28fWrVqhX379iE7OxvZ2dnYt28f2rRpgx07duDw4cO4e/cupkyZUh71ElEVNnbsWACQQky3bt2QkJCAbt26ydp1/YjI9Ok9IjNhwgSsXr0awcHBUlunTp1gbW2NUaNG4dy5c1iyZEmlu3qJiJTv0aNH0teurq544403ULt2bbzxxhs4fvw47t69K/XjVUtEVYPeQSYlJaXUy6AcHR2lG7nVr18fd+7cefnqiIge07NnTwCAj48Pbt68WWKyb506dXDt2jX07NkTv//+u7HKJKIKpPeppRYtWmDq1Kmy89O3b9/GtGnT0KpVKwBAcnIyvL29DVclERGA69evAwB++OEHpKSkwN7eHmq1Gvb29khJScGGDRtk/YjI9Ok9IrN27Vr06dMHtWrVksLKjRs3ULduXWzfvh0A8ODBA3zyySeGrZSIqrzatWvjxo0baNeunWwpiAcPHqBOnTpQq9VSPyKqGsq8su/jtFot9u7diz///BMA0KBBA3Tp0kX6JVKZcGVfItORnZ0NZ2dnadvX1xcLFizAtGnTcPXqVak9KysLTk5ORqiQiAzF4Cv7Pk6tVqNbt27SlQJERBXh8cm+wN/z8Tw8PFC/fn1ZkHn06BGDDFEV8UIjMnFxcYiLi8OtW7dKrPT77bffGqw4Q+CIDJHp8PT0RGZmJszNzaUbRj5O1+7h4SG7sS0RKU9ZP7/1Phc0a9YsdO3aFXFxcbhz5w7u378vexARlZesrCwAf0/2vXnzJlxcXGBubg4XFxfcvHkT3333nawfEZk+vUdkvLy8sGDBAgwZMqS8ajIojsgQmQ7diIylpSUKCwtL7LewsEBRURFHZIhMQLmNyBQWFsoWwyMiqiinTp0CACnEDBkyBKdPn5b+sCoqKpL1IyLTp3eQGTlyJDZt2lQetRARPZO9vb1s+/Dhw0hKSsLhw4ef2Y+ITJfeVy3l5+dj9erV2L9/P5o0aQILCwvZ/i+//NJgxRERPU438mJmZgaNRoNr165h0KBB0n5d+5AhQ/DTTz8Zq0wiqkB6B5kzZ86gWbNmAICkpCTZPpVKZZCiiIhKk5KSAgA4efIkrKys0LhxY2g0GpiZmeHcuXPIy8tDixYtpH5EZPr0DjIHDhwojzqIiJ6rXr16OHv2LNq1a4cHDx5I7RqNBv7+/tIppXr16hmrRCKqYC+8FO/ly5exZ88eaYGqF1iOhohIL7p7KelCTLdu3ZCQkCAtzqlr1/UjItOnd5C5e/cuOnXqhFdeeQU9evRAeno6AGDEiBGYPHmywQskItLRaDSybVdXV1haWsLV1fWZ/YjIdOkdZCIjI2FhYYHr16/D1tZWan/77bexe/dugxZHRPS4nj17AoC0psTGjRvRokULbNy4EQDg4OAg60dEpk/vILN3717Mnz8ftWrVkrXXr18f165dM1hhRERPun79OgBgz549uHv3LgICAlCtWjUEBATg7t272Llzp6wfEZk+vYNMXl6ebCRG5969e7CysnrhQj7//HOoVCpMnDhRasvPz0dERARcXV1hb2+Pfv36ITMz84Vfg4iUrXbt2gCAd955Bx4eHkhKSsK9e/eQlJQEDw8P6fJsXT8iMn16B5mQkBCsX79e2lapVNBqtViwYAFCQ0NfqIjjx49j1apVaNKkiaw9MjISv/zyC7Zs2YJDhw4hLS0Nffv2faHXICLl0424pKamwsXFBWvWrEF6ejrWrFkDFxcXaVRY14+ITJ/el18vWLAAnTp1wokTJ1BYWIhp06bh3LlzuHfvHo4cOaJ3AQ8ePMCgQYOwZs0azJkzR2rPzs7G2rVrsWnTJnTs2BEAsG7dOjRs2BDHjh1D27Zt9X4tIlI2Gxsb6evbt29jy5YtaNCgAbZs2YLbt2+X2o+ITJveIzIBAQH4888/0b59e/Tp0wd5eXno27cv/vvf/77Q2g0RERHo2bMnOnfuLGtPTExEUVGRrN3f3x+1a9dGQkLCU49XUFCAnJwc2YOITMPy5csBAG5ubgD+nrP32muvYe/evbJ2XT8iMn16j8gAgJOTEz7++GNZ282bNzFq1CisXr26zMfZvHkzTp48iePHj5fYl5GRAUtLSzg7O8van3dX23nz5mHWrFllroGIlEO3Ym9ubm6p+3XtXNmXqOp44QXxnnT37l2sXbu2zP1v3LiBCRMmYOPGjbC2tjZUGYiKikJ2drb0uHHjhsGOTUTGpRv1zc/PBwD4+vpiy5Yt8PX1lbVzZV+iqsNgQUZfiYmJuHXrFpo3bw5zc3OYm5vj0KFD+Prrr2Fubg4PDw8UFhYiKytL9rzMzEx4eno+9bhWVlZwdHSUPYjINDw+2f/27du4cuUK+vfvjytXrsjmyPCiAKKqw2hBplOnTjh79ixOnTolPVq2bIlBgwZJX1tYWCAuLk56zqVLl3D9+nUEBQUZq2wiMqLWrVtLXwcEBGD16tVIS0vD6tWrERAQUGo/IjJtLzRHxhAcHBxkv3gAwM7ODq6urlL7iBEjMGnSJFSrVg2Ojo4YP348goKCeMUSURWlG6H9xz/+gZ07d2L06NHSPnNzc/To0QO//vpriZFcIjJdZQ4yzxuqLY9fHIsXL4ZarUa/fv1QUFCAsLAwXo1AVIU5OzsjMzMTu3btKnE/peLiYuzbt0/qR0RVg0qU8bbVw4cPL9MB161b91IFGVpOTg6cnJyQnZ3N+TJECpeRkQEvLy9pe8iQIZgyZQq++OIL2R2v09PTnzmXjogqv7J+fpd5RKayBRQiqnrs7e1l24cPH0a3bt1w+PDhZ/YjItNltMm+RET60t1LSa3++1fXtWvXMGjQIOnWBLp2XT8iMn0MMkSkGLqF7v773//i6NGjsn1Hjx6VFtfkgnhEVYfRrloiItJXvXr1cPbsWTRt2rTEvuDgYFk/IqoaOCJDRIrx+IReAHB0dMTXX39dYiLgk/2IyHTpHWQOHz6M4uLiEu3FxcUlJtwRERnS46v3AkDv3r3Rrl079O7d+5n9iMh0lfnyax0zMzOkp6fD3d1d1n737l24u7uXWNvB2Hj5NZHpsLGxQX5+PtRqNbRabYn9KpUKQghYW1vj0aNHRqiQiAylrJ/feo/ICCGgUqlKtN+9exd2dnb6Ho6IqMwKCgoAAEuWLEFWVhbatWsHb29vtGvXDllZWVi4cKGsHxGZvjKPyOhW9t2+fTu6desGKysraZ9Go8GZM2fQoEED7N69u3wqfUEckSEyHboRGVtbW7i7uyM1NVXa5+Pjg4yMDOTn53NEhsgEGHxExsnJCU5OThBCwMHBQdp2cnKCp6cnRo0ahe+//94gxRMRleb8+fMAgIcPH8LPzw8JCQnIzc1FQkIC/Pz8kJ+fL+tHRKZP7zkys2bNwpQpUxRzGokjMkSmQ6PRwNz8f6tGODg4IDo6GrNmzUJubq7UXlxcDDMzM2OUSEQGUm5zZN5+++2nhpg9e/boezgiojKLj48H8L8VfHNzczFlyhQpxOjadf2IyPTpHWSaN2+OZcuWydoKCgowbtw49OnTx2CFERE9KT09HQCQnZ2NHTt2yPbt2LEDWVlZsn5EZPr0Xtk3JiYGY8aMwc6dO7Fu3Tqkp6dj4MCB0Gq1/CuIiMqV7s7XDg4OJfb16tWrRD8iMn16j8i89dZbOH36NIqKitC4cWMEBQXh9ddfx8mTJ9GqVavyqJGICAAQEhIi27awsMCnn34KCwuLZ/YjItP1wrcoKCwshEajgUajgZeXF6ytrQ1ZFxFRCZcuXZK+7tChAw4dOoSpU6fi0KFD6NChQ6n9iMi06R1kNm/ejMDAQDg5OeHPP//Ezp07sXr1aoSEhODKlSvlUSMREQAgMDAQwN8rjKempiI4OBiOjo4IDg7GtWvXpCuVdP2IyPTpHWRGjBiBuXPn4ueff4abmxu6dOmCs2fPombNmmjWrFk5lEhE9DfdbQmmT5+O3bt3S8HFzMwMu3btwuTJk2X9iMj06b2OzKVLl9CgQYNS923YsAFDhgwxSGGGwnVkiEyHmZlZmUKKWq2udPd9IyL9lNs6Mg0aNEBxcTH279+PVatWSes3pKWl4Y033njxiomInuPs2bOybVtbWyxcuBC2trbP7EdEpkvvy6+vXbuGbt264fr16ygoKECXLl3g4OCA+fPno6CgACtXriyPOomIYGlpKdsuKirC7du3UVRU9Mx+RGS69B6RmTBhAlq2bIn79+/DxsZGan/jjTcQFxdn0OKIiB7XqFEj2XZRUREWLFhQIsg82Y+ITJfeIzLx8fE4evRoib94fHx88NdffxmsMCKiJ+kCy8KFC+Hi4oKRI0dK+/71r38hIyMDn3zySYlgQ0SmS+8go9VqS51Ed/PmzVJX2yQiMhQLCwsUFRVh6tSpJfY9HmqeXCCPiEyX3qeWunbtiiVLlkjbKpUKDx48QHR0NHr06GHI2oiIZM6fP1+i7a233ipTPyIyTWUOMmZmZrh16xYWLVqEI0eOoFGjRsjPz8fAgQOl00rz588vz1qJqIp78vS1ra0tmjRpUuKqJZ7mJqo6yryOjFqtRkZGBtzd3VFcXIzNmzfjzJkzePDgAZo3b45BgwbJJv9WFlxHhsh0qFSqMvfVc4ksIqpkyvr5rfccGQAwNzfH4MGDX7g4IqKX8dZbb2HQoEHo06eP1LZ9+3asXbsWP//8sxErI6KKpteIzJw5c2Bvb//Mfh9++KFBCjMUjsgQmQ6OyBBVHeUyIrNy5Urp3ialUalUlS7IEJHpOHjwoOwu1xYWFvjoo4/w+eefyy65PnjwYMUXR0RGoVeQOXHiBNzd3curFiKiZ3Jzc5NtFxUV4fTp0yXWjXmyHxGZrjJftaTPkC4RUXkIDAws0VbanJjS+hGRaSpzkOH5ZiIytrLc+VqffkSkfGUOMtHR0c+d6EtEVJ7U6pK/soYNG1amfkRkmvQKMk8uOkVEVJG+/fZb6eu4uDgIIbBu3ToIIWQ3rX28HxGZtjJffq1UvPyayHQ8OVfPwsICkZGRWLx4cYkJvyb+q43I5JX185tBhogUg+vIEFUdZf38NuqJ5BUrVqBJkyZwdHSEo6MjgoKCsGvXLml/fn4+IiIi4OrqCnt7e/Tr1w+ZmZlGrJiIiIgqE72DTHR0NK5du2aQF69VqxY+//xzJCYm4sSJE+jYsSP69OmDc+fOAQAiIyPxyy+/YMuWLTh06BDS0tLQt29fg7w2ESnP0aNHS7SVNtm3tH5EZJr0DjLbt29HvXr10KlTJ2zatAkFBQUv/OK9e/dGjx49UL9+fbzyyiv47LPPYG9vj2PHjiE7Oxtr167Fl19+iY4dO6JFixZYt24djh49imPHjr3waxKRacnPzzd2CURkRHoHmVOnTuH48eNo3LgxJkyYAE9PT4wZMwbHjx9/qUI0Gg02b96MvLw8BAUFITExEUVFRejcubPUx9/fH7Vr10ZCQsJTj1NQUICcnBzZg4hMQ3BwcIm2zZs3l6kfEZmmF5oj8+qrr+Lrr79GWloa1q5di5s3b6Jdu3Zo0qQJvvrqK2RnZ5f5WGfPnoW9vT2srKzwwQcf4KeffkKjRo2QkZEBS0tLODs7y/p7eHggIyPjqcebN28enJycpIe3t/eLfItEVIkNGzYMn3zyiaztk08+wYABA4xUEREZy0tN9hVCoKioCIWFhRBCwMXFBUuXLoW3tzd+/PHHMh2jQYMGOHXqFP744w+MGTMGQ4cOxfnz51+4pqioKGRnZ0uPGzduvPCxiKhyiomJwZw5c2Rtc+bMKXV0hohM2wsFmcTERIwbNw5eXl6IjIzEq6++igsXLuDQoUNITk7GZ599Vua7YFtaWsLPzw8tWrTAvHnz0LRpU3z11Vfw9PREYWEhsrKyZP0zMzPh6en51ONZWVlJV0HpHkRkGkqbxNukSZMy9SMi06R3kAkMDETbtm1x9epVrF27Fjdu3MDnn38OPz8/qc8777yD27dvv1BBWq0WBQUFaNGiBSwsLGSrdV66dAnXr19HUFDQCx2biJTt8uXLJdoaNWpUpn5EZJr0XhDvn//8J9577z3UrFnzpV88KioK3bt3R+3atZGbm4tNmzZh/vz52LNnD7p06YIxY8bg119/RUxMDBwdHTF+/HgA+v21xQXxiEwHF8QjqjrKZUG8oqIixMTEGOxKoFu3buHdd99FgwYN0KlTJxw/flwKMQCwePFi9OrVC/369cNrr70GT09PxMbGGuS1iUi5mjRpUupkX39/fyNVRETGoveITM2aNbF//340bNiwvGoyKI7IEJkOjsgQVR3ldouCiIgIzJ8/H8XFxS9VIBGRvtavX1+irbTJvqX1IyLTZK7vE44fP464uDjs3bsXgYGBsLOzk+3nqR8iqkiFhYXGLoGIjEjvU0vDhw9/5v5169a9VEGGxlNLRKaDp5aIqo6yfn7rPSJT2YIKEVU9TZo0wZQpU/Duu+9KbevXr8fcuXNx8eJFI1ZGRBVN7xEZACguLsbBgweRkpKCgQMHwsHBAWlpaXB0dIS9vX151PnCOCJDZDoeH5Ep7VfX8/YTkXKU22Tfa9euITAwEH369EFERIS08N38+fMxZcqUF6+YiOg5Hp/Ee/DgQdm+x7c52Zeo6tB7RCY8PBwODg5Yu3YtXF1dcfr0adStWxcHDx7E+++/j+Tk5PKq9YVwRIbItDw5T8bf37/E6SSOxhApX7nNkYmPj8fRo0dhaWkpa/fx8cFff/2lf6VEVKU8fPjwpeaxJCYmokWLFtL2k8dKTEzEyZMnX/j4/v7+sLW1feHnE1HF0jvIaLVaaDSaEu03b96Eg4ODQYoiItN18eJFWRAxtJc9dmJiIpo3b26gaoiovOkdZLp27YolS5Zg9erVAP4e5n3w4AGio6PRo0cPgxdIRKbF398fiYmJBjnWhQsXMHjwYHz//fcGW22ctzkgUha9g8yiRYsQFhaGRo0aIT8/HwMHDkRycjKqV6+OH374oTxqJCITYmtra/ARj4YNG3IUhaiK0jvI1KpVC6dPn8bmzZtx5swZPHjwACNGjMCgQYNgY2NTHjUSERERlUrvIAMA5ubmGDx4sKFrISIiItKL3kHmeeszPL7SJhEREVF50jvITJgwQbZdVFSEhw8fwtLSEra2tgwyREREVGH0Xtn3/v37sseDBw9w6dIltG/fnpN9iYiIqELpHWRKU79+fXz++eclRmuIiIiIypNBggzw9wTgtLQ0Qx2OiIiI6Ln0niPz888/y7aFEEhPT8fSpUvRrl07gxVGRERE9Dx6B5nw8HDZtkqlgpubGzp27IhFixYZqi4iIiKi53qhey0RERERVQYvPEfmzp07yMnJMWQtRERERHrRK8hkZWUhIiIC1atXh4eHB1xcXODp6YmoqCg8fPiwvGokIiIiKlWZTy3du3cPQUFB+OuvvzBo0CDpTrPnz5/HN998g3379uH333/HmTNncOzYMXz44YflVjQRERERoEeQmT17NiwtLZGSkgIPD48S+7p27YohQ4Zg7969+Prrrw1eKBEREdGTyhxktm3bhlWrVpUIMQDg6emJBQsWoEePHoiOjsbQoUMNWiQRERFRaco8RyY9PR2NGzd+6v6AgACo1WpER0cbpDAiIiKi5ylzkKlevTpSU1Ofuv/q1atwd3c3RE1EREREZVLmIBMWFoaPP/4YhYWFJfYVFBTg008/Rbdu3QxaHBEREdGz6DXZt2XLlqhfvz4iIiLg7+8PIQQuXLiA5cuXo6CgAOvXry/PWomIiIhkyhxkatWqhYSEBIwdOxZRUVEQQgD4+xYFXbp0wdKlS1G7du1yK5SIiIjoSXrdosDX1xe7du3C/fv3kZycDADw8/NDtWrVyqU4IiIiomfR+15LAODi4oLWrVsbuhYiIiIivbzwvZaIiIiIjI1BhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBTLqEFm3rx5aNWqFRwcHODu7o7w8HBcunRJ1ic/Px8RERFwdXWFvb09+vXrh8zMTCNVTERERJWJUYPMoUOHEBERgWPHjmHfvn0oKipC165dkZeXJ/WJjIzEL7/8gi1btuDQoUNIS0tD3759jVg1ERERVRYvtI6MoezevVu2HRMTA3d3dyQmJuK1115DdnY21q5di02bNqFjx44AgHXr1qFhw4Y4duwY2rZta4yyiYiIqJKoVHNksrOzAUBaKTgxMRFFRUXo3Lmz1Mff3x+1a9dGQkJCqccoKChATk6O7EFERESmqdIEGa1Wi4kTJ6Jdu3YICAgAAGRkZMDS0hLOzs6yvh4eHsjIyCj1OPPmzYOTk5P08Pb2Lu/SiYiIyEgqTZCJiIhAUlISNm/e/FLHiYqKQnZ2tvS4ceOGgSokIiKiysaoc2R0xo0bhx07duDw4cOoVauW1O7p6YnCwkJkZWXJRmUyMzPh6elZ6rGsrKxgZWVV3iUTERFRJWDUERkhBMaNG4effvoJv/32G3x9fWX7W7RoAQsLC8TFxUltly5dwvXr1xEUFFTR5RIREVElY9QRmYiICGzatAnbt2+Hg4ODNO/FyckJNjY2cHJywogRIzBp0iRUq1YNjo6OGD9+PIKCgnjFEhERERk3yKxYsQIA0KFDB1n7unXrMGzYMADA4sWLoVar0a9fPxQUFCAsLAzLly+v4EqJiIioMjJqkBFCPLePtbU1li1bhmXLllVARURERKQkleaqJSIiIiJ9McgQERGRYjHIEBERkWIxyBAREZFiMcgQERGRYjHIEBERkWIxyBAREZFiMcgQERGRYjHIEBERkWIxyBAREZFiMcgQERGRYjHIEBERkWIxyBAREZFiMcgQERGRYjHIEBERkWIxyBAREZFimRu7ACJSjuTkZOTm5hq7DMmFCxdk/60sHBwcUL9+fWOXQVQlMMgQUZkkJyfjlVdeMXYZpRo8eLCxSyjhzz//ZJghqgAMMkRUJrqRmO+//x4NGzY0cjV/e/ToEVJTU+Hj4wMbGxtjlwPg79GhwYMHV6qRKyJTxiBDRHpp2LAhmjdvbuwyJO3atTN2CURkRJzsS0RERIrFIENERESKxSBDREREisUgQ0RERIrFIENERESKxSBDREREisUgQ0RERIrFIENERESKxSBDREREisUgQ0RERIrFIENERESKxSBDREREisUgQ0RERIrFIENERESKxSBDREREisUgQ0RERIrFIENERESKxSBDREREimXUIHP48GH07t0bNWrUgEqlwrZt22T7hRCYMWMGvLy8YGNjg86dOyM5Odk4xRIREVGlY9Qgk5eXh6ZNm2LZsmWl7l+wYAG+/vprrFy5En/88Qfs7OwQFhaG/Pz8Cq6UiIiIKiNzY7549+7d0b1791L3CSGwZMkSfPLJJ+jTpw8AYP369fDw8MC2bdswYMCAiiyViIiIKiGjBplnuXr1KjIyMtC5c2epzcnJCW3atEFCQsJTg0xBQQEKCgqk7ZycnHKvlaiq8LRXwSbrTyCN0+uexibrT3jaq4xdBlGVUWmDTEZGBgDAw8ND1u7h4SHtK828efMwa9ascq2NqKoa3cISDQ+PBg4bu5LKqyH+fp+IqGJU2iDzoqKiojBp0iRpOycnB97e3kasiMh0rEosxNszYtDQ39/YpVRaFy5exKpFA/EPYxdCVEVU2iDj6ekJAMjMzISXl5fUnpmZiWbNmj31eVZWVrCysirv8oiqpIwHAo+cXwFqNDN2KZXWowwtMh4IY5dBVGVU2hPdvr6+8PT0RFxcnNSWk5ODP/74A0FBQUasjIiIiCoLo47IPHjwAJcvX5a2r169ilOnTqFatWqoXbs2Jk6ciDlz5qB+/frw9fXFp59+iho1aiA8PNx4RRMREVGlYdQgc+LECYSGhkrburktQ4cORUxMDKZNm4a8vDyMGjUKWVlZaN++PXbv3g1ra2tjlUxERESViFGDTIcOHSDE088lq1QqzJ49G7Nnz67AqoiIiEgpKu0cGSIiIqLnYZAhIiIixWKQISIiIsVikCEiIiLFYpAhIiIixWKQISIiIsWqtLcoIKLK5eHDhwCAkydPGrmS/3n06BFSU1Ph4+MDGxsbY5cDALhw4YKxSyCqUhhkiKhMLl68CAB4//33jVyJMjg4OBi7BKIqgUGGiMpEd2sQf39/2NraGreY/+/ChQsYPHgwvv/+ezRs2NDY5UgcHBxQv359Y5dBVCUwyBBRmVSvXh0jR440dhmlatiwIZo3b27sMojICDjZl4iIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBSLQYaIiIgUi0GGiIiIFItBhoiIiBTL3NgFEFHV8vDhQ1y8eNEgx7pw4YLsv4bg7+8PW1tbgx2PiMoXgwwRVaiLFy+iRYsWBj3m4MGDDXasxMRENG/e3GDHI6LyxSBDRBXK398fiYmJBjnWo0ePkJqaCh8fH9jY2BjkmP7+/gY5DhFVDJUQQhi7iOdZtmwZFi5ciIyMDDRt2hTffPMNWrduXabn5uTkwMnJCdnZ2XB0dCznSomIiMgQyvr5Xekn+/7444+YNGkSoqOjcfLkSTRt2hRhYWG4deuWsUsjIiIiI6v0IzJt2rRBq1atsHTpUgCAVquFt7c3xo8fj48++qhE/4KCAhQUFEjbOTk58Pb25ogMERGRgpjEiExhYSESExPRuXNnqU2tVqNz585ISEgo9Tnz5s2Dk5OT9PD29q6ocomIiKiCVeogc+fOHWg0Gnh4eMjaPTw8kJGRUepzoqKikJ2dLT1u3LhREaUSERGREZjcVUtWVlawsrIydhlERERUASr1iEz16tVhZmaGzMxMWXtmZiY8PT2NVBURERFVFpU6yFhaWqJFixaIi4uT2rRaLeLi4hAUFGTEyoiIiKgyqPSnliZNmoShQ4eiZcuWaN26NZYsWYK8vDwMHz7c2KURERGRkVX6IPP222/j9u3bmDFjBjIyMtCsWTPs3r27xARgIiIiqnoq/ToyL4sr+xIRESmPSawjQ0RERPQsDDJERESkWAwyREREpFiVfrLvy9JNAcrJyTFyJURERFRWus/t503lNfkgk5ubCwC85xIREZEC5ebmwsnJ6an7Tf6qJa1Wi7S0NDg4OEClUhm7HCIyIN3d7W/cuMGrEolMjBACubm5qFGjBtTqp8+EMfkgQ0Smi8srEBEn+xIREZFiMcgQERGRYjHIEJFiWVlZITo6GlZWVsYuhYiMhHNkiIiISLE4IkNERESKxSBDREREisUgQ0RERIrFIENERESKxSBDREREisUgQ0SKc/jwYfTu3Rs1atSASqXCtm3bjF0SERkJgwwRKU5eXh6aNm2KZcuWGbsUIjIyk7/7NRGZnu7du6N79+7GLoOIKgGOyBAREZFiMcgQERGRYjHIEBERkWIxyBAREZFiMcgQERGRYvGqJSJSnAcPHuDy5cvS9tWrV3Hq1ClUq1YNtWvXNmJlRFTRVEIIYewiiIj0cfDgQYSGhpZoHzp0KGJiYiq+ICIyGgYZIiIiUizOkSEiIiLFYpAhIiIixWKQISIiIsVikCEiIiLFYpAhIiIixWKQISIiIsVikCEiIiLFYpAhIiIixWKQISIiIsVikCEiIiLFYpAhIiIixfp/VIR7YRX/BG4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  70\n",
      "Min length:  1\n",
      "Average length:  11.742588235294118\n",
      "Standard deviation:  6.8770212128729264\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_df = pd.read_json(data_path, lines=True)\n",
    "# Calculate the lengths\n",
    "query_lengths = train_df['text'].apply(lambda n: len(n.split()))\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(query_lengths, bins=30, alpha=0.7, color='b')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Distribution of Query Text Lengths')\n",
    "plt.xlabel('Query Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Create a boxplot\n",
    "plt.boxplot(query_lengths)\n",
    "plt.title('Boxplot of Query Text Lengths')\n",
    "plt.ylabel('Query Text Length')\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"Max length: \", query_lengths.max())\n",
    "print(\"Min length: \", query_lengths.min())\n",
    "print(\"Average length: \", query_lengths.mean())\n",
    "print(\"Standard deviation: \", query_lengths.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HatefulMemesData(Dataset):\n",
    "    def __init__(self, df,img_dir, tokenizer, sequence_length,caption_sequence_length=512, visual_embed_model='vit', print_text=False, visual_embeder_detecron2=None, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):         \n",
    "        self.device=device\n",
    "        self.sequence_length = sequence_length\n",
    "        self.caption_sequence_length= caption_sequence_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_text = print_text\n",
    "        self.dataset = pd.read_json(path_or_buf=df, lines=True).to_dict(orient='records')\n",
    "        self.img_dir = img_dir\n",
    "        self.visual_embed_model = visual_embed_model\n",
    "        self.feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.feature_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k').to(self.device)\n",
    "        if self.visual_embed_model=='detectron2' and visual_embeder_detecron2 is not None:\n",
    "            self.visualembedder=visual_embeder_detecron2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "    def tokenize_data(self, example):\n",
    "   \n",
    "        idx = example['id']\n",
    "        # idx = [idx] if isinstance(idx, str) else idx\n",
    "        \n",
    "        encoded_dict = self.tokenizer(example['text'], padding='max_length', max_length=self.sequence_length, truncation=True, return_tensors='pt')\n",
    "        tokens = encoded_dict['input_ids']\n",
    "        token_type_ids = encoded_dict['token_type_ids']\n",
    "        attn_mask = encoded_dict['attention_mask']\n",
    "        \n",
    "        captioning_encode_dict=self.tokenizer(example['query_1'], padding='max_length', max_length=self.caption_sequence_length,truncation=True, return_tensors='pt')\n",
    "        caption_token=captioning_encode_dict['input_ids']\n",
    "        caption_token_type_ids=captioning_encode_dict['token_type_ids']\n",
    "        caption_attn_mask=captioning_encode_dict['attention_mask']\n",
    "\n",
    "        targets = torch.tensor(example['label']).type(torch.int64)\n",
    "\n",
    "        ## Get Visual Embeddings\n",
    "        try:\n",
    "            if self.visual_embed_model=='vit':\n",
    "                #TODO: make it work\n",
    "                img = example['img'].split('/')[-1]\n",
    "                img = Image.open(os.path.join(self.img_dir , img))\n",
    "                img = np.array(img)\n",
    "                img = img[...,:3]\n",
    "                inputs = self.feature_extractor(images=img, return_tensors=\"pt\")\n",
    "                outputs = self.feature_model(**inputs.to(device))\n",
    "                visual_embeds = outputs.last_hidden_state\n",
    "                visual_embeds = visual_embeds.cpu() #\n",
    "            elif self.visual_embed_model=='detectron2':\n",
    "                visual_embeds = self.visualembedder.visual_embeds_detectron2([cv2.imread(os.path.join(self.img_dir, example['img'].split('/')[-1]))])[0]\n",
    "\n",
    "        except:\n",
    "            # print(\"Error with Id: \", idx)\n",
    "            if self.visual_embed_model=='vit':\n",
    "                visual_embeds = np.zeros(shape=(197, 768), dtype=float)\n",
    "            elif self.visual_embed_model=='detectron2':\n",
    "                visual_embeds = np.zeros(shape=(100, 1024), dtype=float)\n",
    "\n",
    "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "        visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.int64)\n",
    "\n",
    "        inputs={\"input_ids\": tokens.squeeze(),\n",
    "            \"attention_mask\": attn_mask.squeeze(),\n",
    "            \"token_type_ids\": token_type_ids.squeeze(),\n",
    "            \"visual_embeds\": visual_embeds.squeeze(),\n",
    "            \"visual_token_type_ids\": visual_token_type_ids.squeeze(),\n",
    "            \"visual_attention_mask\": visual_attention_mask.squeeze(),\n",
    "            \"label\": targets.squeeze(),\n",
    "            \"caption_input_ids\": caption_token.squeeze(),\n",
    "            \"caption_attention_mask\": caption_attn_mask.squeeze(),\n",
    "            \"caption_token_type_ids\": caption_token_type_ids.squeeze()\n",
    "        }\n",
    "        \n",
    "        return inputs\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenize_data(self.dataset[index])\n",
    "        \n",
    "        if self.print_text:\n",
    "            for k in inputs.keys():\n",
    "                print(k, inputs[k].shape, inputs[k].dtype)\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture (with weight ensemble fusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, VisualBertModel, TrainingArguments, Trainer, VisualBertConfig\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "class VisualBERTClassifier(torch.nn.Module):\n",
    "    def __init__(self, visual_embedder='vit'):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(VisualBERTClassifier, self).__init__()\n",
    "        configuration = VisualBertConfig.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre',\n",
    "                                                hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1)\n",
    "        self.visualbert = VisualBertModel.from_pretrained('uclanlp/visualbert-nlvr2-coco-pre', config=configuration)\n",
    "        self.bertmodel = BertModel.from_pretrained('bert-base-uncased')\n",
    "        for param in self.visualbert.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.bertmodel.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if visual_embedder=='vit':\n",
    "            self.embed_cls = nn.Linear(768, 1024)\n",
    "        elif visual_embedder=='detectron2':\n",
    "            self.embed_cls = nn.Linear(1024, 1024)\n",
    "        self.num_labels = 2\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5), requires_grad=True)  # Initial value of alpha\n",
    "        self.cls= nn.Sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(768, 348),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(348, 192),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(192, self.num_labels)\n",
    "        )\n",
    "        # TODO: Calculate the weights for the loss function and weight balanced loss\n",
    "        # nSamples = [5178, 2849]\n",
    "        # normedWeights = [1 - (x / sum(nSamples)) for x in nSamples]\n",
    "        # self.loss_fct = CrossEntropyLoss(weight=torch.FloatTensor(normedWeights))\n",
    "        self.loss_fct = CrossEntropyLoss()\n",
    "        \n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask,\n",
    "                visual_token_type_ids, labels,caption_input_ids, caption_attention_mask, caption_token_type_ids):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        \n",
    "        visual_embeds_cls = self.embed_cls(visual_embeds)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.visualbert(\n",
    "                    input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    visual_embeds=visual_embeds_cls,\n",
    "                    visual_attention_mask=visual_attention_mask,\n",
    "                    visual_token_type_ids=visual_token_type_ids,\n",
    "                )\n",
    "        \n",
    "        visualbert_embedding = outputs[1] # output is a context vector of 768 dimensions\n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            caption_outputs = self.bertmodel(caption_input_ids, attention_mask=caption_attention_mask, token_type_ids=caption_token_type_ids)\n",
    "                \n",
    "        # Get the embeddings of the [CLS] token\n",
    "        caption_embeddings = caption_outputs.last_hidden_state[:,0,:] # output is a context vector of 768 dimensions\n",
    "\n",
    "        # funsion model: weight ensenble of the two embeddings: alpha*visualbert_embedding + (1-alpha)*caption_embeddings \n",
    "        fused_embedding = self.alpha * self.dropout(visualbert_embedding) + (1-self.alpha) * self.dropout(caption_embeddings)\n",
    "\n",
    "        logits = self.cls(fused_embedding)\n",
    "        ##\n",
    "        \n",
    "        reshaped_logits = logits.view(-1, self.num_labels)\n",
    "        loss = self.loss_fct(reshaped_logits, labels.view(-1))\n",
    "      \n",
    "        return loss, reshaped_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To check model architecture and shape for each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha torch.Size([])\n",
      "visualbert.embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
      "visualbert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "visualbert.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "visualbert.embeddings.LayerNorm.weight torch.Size([768])\n",
      "visualbert.embeddings.LayerNorm.bias torch.Size([768])\n",
      "visualbert.embeddings.visual_token_type_embeddings.weight torch.Size([2, 768])\n",
      "visualbert.embeddings.visual_position_embeddings.weight torch.Size([512, 768])\n",
      "visualbert.embeddings.visual_projection.weight torch.Size([768, 1024])\n",
      "visualbert.embeddings.visual_projection.bias torch.Size([768])\n",
      "visualbert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "visualbert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "visualbert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "visualbert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "visualbert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "visualbert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "visualbert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "visualbert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "visualbert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "visualbert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "visualbert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "visualbert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "visualbert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "visualbert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "visualbert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "visualbert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "visualbert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "visualbert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "visualbert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "visualbert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "visualbert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "visualbert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "visualbert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "visualbert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "visualbert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "visualbert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "visualbert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "visualbert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "visualbert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "visualbert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "visualbert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "visualbert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "visualbert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "visualbert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "visualbert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "visualbert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "visualbert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "visualbert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "visualbert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "visualbert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "visualbert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "visualbert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "visualbert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "visualbert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "visualbert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "visualbert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "visualbert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "visualbert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "visualbert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "visualbert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "visualbert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "visualbert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "visualbert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "visualbert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "visualbert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "visualbert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "visualbert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "visualbert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "visualbert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "visualbert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "visualbert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "visualbert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "visualbert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "visualbert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "visualbert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "visualbert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "visualbert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "visualbert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "visualbert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "visualbert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "visualbert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "visualbert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "visualbert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "visualbert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "visualbert.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "visualbert.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "visualbert.pooler.dense.weight torch.Size([768, 768])\n",
      "visualbert.pooler.dense.bias torch.Size([768])\n",
      "bertmodel.embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
      "bertmodel.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bertmodel.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "bertmodel.embeddings.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.embeddings.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bertmodel.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bertmodel.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bertmodel.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bertmodel.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bertmodel.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bertmodel.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bertmodel.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bertmodel.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bertmodel.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bertmodel.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bertmodel.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bertmodel.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bertmodel.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bertmodel.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bertmodel.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bertmodel.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bertmodel.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bertmodel.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bertmodel.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bertmodel.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bertmodel.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bertmodel.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bertmodel.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bertmodel.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bertmodel.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bertmodel.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bertmodel.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bertmodel.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bertmodel.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bertmodel.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bertmodel.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bertmodel.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bertmodel.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bertmodel.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bertmodel.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bertmodel.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bertmodel.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bertmodel.encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "bertmodel.encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "bertmodel.pooler.dense.weight torch.Size([768, 768])\n",
      "bertmodel.pooler.dense.bias torch.Size([768])\n",
      "embed_cls.weight torch.Size([1024, 768])\n",
      "embed_cls.bias torch.Size([1024])\n",
      "cls.0.weight torch.Size([768, 768])\n",
      "cls.0.bias torch.Size([768])\n",
      "cls.3.weight torch.Size([348, 768])\n",
      "cls.3.bias torch.Size([348])\n",
      "cls.6.weight torch.Size([192, 348])\n",
      "cls.6.bias torch.Size([192])\n",
      "cls.9.weight torch.Size([2, 192])\n",
      "cls.9.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "model=VisualBERTClassifier()\n",
    "for layer_name, params in model.named_parameters():\n",
    "    print(layer_name, params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for f1 contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/f1/f1.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for precision contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/precision/precision.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for recall contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/recall/recall.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from datasets import load_metric\n",
    "acc_metric = load_metric('accuracy')\n",
    "f1_metric = load_metric('f1')\n",
    "precision_metric = load_metric('precision')\n",
    "recall_metric = load_metric('recall')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)\n",
    "    auc_score = roc_auc_score(labels, predictions)\n",
    "    return {\"accuracy\": acc['accuracy'], \"auroc\": auc_score,'f1':f1['f1'],'precision':precision['precision'],'recall':recall['recall']} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define training hyperparameters and all model used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/cchang/wandb.key\", \"r\") as f:\n",
    "        wandb_key = f.read().strip()\n",
    "        \n",
    "wandb.login(key=wandb_key)\n",
    "wandb.init(\n",
    "    project=\"HatefulMemes\",\n",
    ")\n",
    "\n",
    "data_folder_path='/scratch/izar/cchang/EE559'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_data_path=os.path.join(data_folder_path, '/data/hateful_memes/train_df_wQuery_.jsonl')\n",
    "validation_data_path=os.path.join(data_folder_path, '/data/hateful_memes/dev_seen_df_wQuery_.jsonl')\n",
    "img_inpainted_dir=os.path.join(data_folder_path, '/data/hateful_memes/img_inpainted')\n",
    "visual_embed_model='vit'\n",
    "## For visual_embed_model='detectron2'\n",
    "# cfg_path=\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
    "# MIN_BOXES=10 \n",
    "# MAX_BOXES=100\n",
    "# visualembedder_detectron2=VisualEmbedder(cfg_path=cfg_path, min_boxes=MIN_BOXES, max_boxes=MAX_BOXES)\n",
    "##\n",
    "# output_dir=os.path.join('/scratch/izar/cchang/EE559/model-checkpoint', f'hatefulmemcladdifier_weightensemble_{visual_embed_model}_{time.strftime(\"%Y%m%d%H%M\")}')\n",
    "output_dir=os.path.join('model-checkpoint', f'hatefulmemcladdifier_weightensemble_{visual_embed_model}_{time.strftime(\"%Y%m%d%H%M\")}')\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 50\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VisualBERTClassifier()\n",
    "model = model.to(device)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = output_dir,\n",
    "    seed = 110, \n",
    "    save_strategy=\"steps\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=1e-3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs= 5,\n",
    "    weight_decay=0.05,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"auroc\",\n",
    "    fp16 = False,\n",
    "    save_steps=100,\n",
    "    eval_steps=50,\n",
    "    logging_steps=50,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset = HatefulMemesData(train_data_path, img_inpainted_dir, tokenizer, sequence_length=seq_len, visual_embed_model=visual_embed_model,device=device),\n",
    "    eval_dataset =  HatefulMemesData(validation_data_path, img_inpainted_dir,tokenizer, sequence_length=seq_len, visual_embed_model=visual_embed_model,device=device),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train + evaluation on validation state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/42500 [05:12<3687:11:40, 312.33s/it]\n",
      "  0%|          | 1/42500 [00:01<22:00:33,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visualbert_embedding torch.Size([1, 768]) torch.float32\n",
      "caption_embeddings torch.Size([1, 768]) torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/42500 [00:03<20:43:29,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visualbert_embedding torch.Size([1, 768]) torch.float32\n",
      "caption_embeddings torch.Size([1, 768]) torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/42500 [00:05<20:41:49,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visualbert_embedding torch.Size([1, 768]) torch.float32\n",
      "caption_embeddings torch.Size([1, 768]) torch.float32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/transformers/trainer.py:2203\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/transformers/trainer.py:3138\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3137\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3138\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3141\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/transformers/trainer.py:3161\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3160\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3161\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3162\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3163\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 70\u001b[0m, in \u001b[0;36mVisualBERTClassifier.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, visual_embeds, visual_attention_mask, visual_token_type_ids, labels, caption_input_ids, caption_attention_mask, caption_token_type_ids)\u001b[0m\n\u001b[1;32m     66\u001b[0m visualbert_embedding \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# output is a context vector of 768 dimensions\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 70\u001b[0m     caption_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbertmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaption_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaption_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaption_token_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Get the embeddings of the [CLS] token\u001b[39;00m\n\u001b[1;32m     73\u001b[0m caption_embeddings \u001b[38;5;241m=\u001b[39m caption_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:,\u001b[38;5;241m0\u001b[39m,:] \u001b[38;5;66;03m# output is a context vector of 768 dimensions\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:988\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    979\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    981\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    982\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    983\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    986\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    987\u001b[0m )\n\u001b[0;32m--> 988\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1000\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1001\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:582\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    572\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    573\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         output_attentions,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    511\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    512\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 514\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    517\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/transformers/pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:527\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    526\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 527\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:439\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 439\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    441\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl_project_310/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inference on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TODO: Inference and evaluate on test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
