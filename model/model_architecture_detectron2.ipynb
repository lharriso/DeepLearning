{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fb195999990>: Failed to establish a new connection: [Errno -2] Name or service not known')': /whl/cpu/torch/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fb195999b40>: Failed to establish a new connection: [Errno -2] Name or service not known')': /whl/cpu/torch/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-2.3.0%2Bcpu-cp310-cp310-linux_x86_64.whl (190.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.4/190.4 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.18.0%2Bcpu-cp310-cp310-linux_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.3.0%2Bcpu-cp310-cp310-linux_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/jessica/.local/lib/python3.10/site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: networkx in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, jinja2, fsspec, torch, torchvision, torchaudio\n",
      "Successfully installed fsspec-2024.2.0 jinja2-3.1.3 mpmath-1.3.0 sympy-1.12 torch-2.3.0+cpu torchaudio-2.3.0+cpu torchvision-0.18.0+cpu\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#For cpu only\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml==5.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (5.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting git+https://github.com/facebookresearch/detectron2.git@v0.5\n",
      "  Cloning https://github.com/facebookresearch/detectron2.git (to revision v0.5) to /tmp/pip-req-build-436w8tp5\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/detectron2.git /tmp/pip-req-build-436w8tp5\n",
      "  Running command git checkout -q 82a57ce0b70057685962b352535147d9a8118578\n",
      "  Resolved https://github.com/facebookresearch/detectron2.git to commit 82a57ce0b70057685962b352535147d9a8118578\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=7.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from detectron2==0.5) (10.3.0)\n",
      "Requirement already satisfied: matplotlib in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from detectron2==0.5) (3.8.4)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from detectron2==0.5) (2.0.7)\n",
      "Requirement already satisfied: termcolor>=1.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from detectron2==0.5) (2.4.0)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from detectron2==0.5) (0.1.8)\n",
      "Requirement already satisfied: tabulate in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from detectron2==0.5) (0.9.0)\n",
      "Requirement already satisfied: cloudpickle in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from detectron2==0.5) (3.0.0)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from detectron2==0.5) (4.66.2)\n",
      "Requirement already satisfied: tensorboard in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from detectron2==0.5) (2.15.2)\n",
      "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from detectron2==0.5) (0.1.5.post20221221)\n",
      "Requirement already satisfied: iopath<0.1.10,>=0.1.7 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from detectron2==0.5) (0.1.9)\n",
      "Collecting future (from detectron2==0.5)\n",
      "  Obtaining dependency information for future from https://files.pythonhosted.org/packages/da/71/ae30dadffc90b9006d77af76b393cb9dfbfc9629f339fc1574a1c52e6806/future-1.0.0-py3-none-any.whl.metadata\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting pydot (from detectron2==0.5)\n",
      "  Obtaining dependency information for pydot from https://files.pythonhosted.org/packages/7f/90/c9b51f3cdff89cd8f93382060330f43d1af098a6624cff439e700791e922/pydot-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading pydot-2.0.0-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: omegaconf>=2.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from detectron2==0.5) (2.3.0)\n",
      "Requirement already satisfied: hydra-core>=1.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from detectron2==0.5) (1.3.2)\n",
      "Collecting black==21.4b2 (from detectron2==0.5)\n",
      "  Obtaining dependency information for black==21.4b2 from https://files.pythonhosted.org/packages/ec/c3/848edbd902fa908e941eaf72dc142b4a5c86e903c1e0129cf7cd098a485b/black-21.4b2-py3-none-any.whl.metadata\n",
      "  Downloading black-21.4b2-py3-none-any.whl.metadata (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.1.2 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from black==21.4b2->detectron2==0.5) (8.1.7)\n",
      "Collecting appdirs (from black==21.4b2->detectron2==0.5)\n",
      "  Obtaining dependency information for appdirs from https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting toml>=0.10.1 (from black==21.4b2->detectron2==0.5)\n",
      "  Obtaining dependency information for toml>=0.10.1 from https://files.pythonhosted.org/packages/44/6f/7120676b6d73228c96e17f1f794d8ab046fc910d781c8d151120c3f1569e/toml-0.10.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting regex>=2020.1.8 (from black==21.4b2->detectron2==0.5)\n",
      "  Obtaining dependency information for regex>=2020.1.8 from https://files.pythonhosted.org/packages/89/67/0176d9c866e74f803e0492f076aa3ccde13c6b4e37efa08d608e03f61b97/regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pathspec<1,>=0.8.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from black==21.4b2->detectron2==0.5) (0.12.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from black==21.4b2->detectron2==0.5) (1.0.0)\n",
      "Requirement already satisfied: numpy in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.5) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.5) (5.1)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from hydra-core>=1.1->detectron2==0.5) (4.9.3)\n",
      "Requirement already satisfied: packaging in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from hydra-core>=1.1->detectron2==0.5) (24.0)\n",
      "Requirement already satisfied: portalocker in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.5) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from matplotlib->detectron2==0.5) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from matplotlib->detectron2==0.5) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from matplotlib->detectron2==0.5) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from matplotlib->detectron2==0.5) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from matplotlib->detectron2==0.5) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from matplotlib->detectron2==0.5) (2.9.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (1.62.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (69.2.0)\n",
      "Requirement already satisfied: six>1.9 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from tensorboard->detectron2==0.5) (3.0.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.5) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.5) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.5) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.5) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.5) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.5) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.5) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.5) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.5) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.5) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/jessica/anaconda3/envs/dl_project_310/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.5) (3.2.2)\n",
      "Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydot-2.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading regex-2024.4.28-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (774 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.1/774.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Building wheels for collected packages: detectron2\n",
      "  Building wheel for detectron2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for detectron2: filename=detectron2-0.5-cp310-cp310-linux_x86_64.whl size=7793252 sha256=a3fb75cd2b7745238d3d7016911481ae64394586511481aa9446120cde83b241\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-avdki68z/wheels/90/18/04/b8a1ce45720c57c3fa0364dc489ef160ee2136361e994d3e81\n",
      "Successfully built detectron2\n",
      "Installing collected packages: appdirs, toml, regex, pydot, future, black, detectron2\n",
      "  Attempting uninstall: black\n",
      "    Found existing installation: black 24.4.2\n",
      "    Uninstalling black-24.4.2:\n",
      "      Successfully uninstalled black-24.4.2\n",
      "  Attempting uninstall: detectron2\n",
      "    Found existing installation: detectron2 0.6\n",
      "    Uninstalling detectron2-0.6:\n",
      "      Successfully uninstalled detectron2-0.6\n",
      "Successfully installed appdirs-1.4.4 black-21.4b2 detectron2-0.5 future-1.0.0 pydot-2.0.0 regex-2024.4.28 toml-0.10.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml==5.1\n",
    "!python -m pip install 'git+https://github.com/facebookresearch/detectron2.git@v0.5'\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.structures.image_list import ImageList\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.modeling.box_regression import Box2BoxTransform\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputs\n",
    "from detectron2.structures.boxes import Boxes\n",
    "from detectron2.layers import nms\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get visual embedding (Detectron2)\n",
    "Reference: https://colab.research.google.com/drive/1bLGxKdldwqnMVA5x4neY7-l_8fKGWQYI?usp=sharing#scrollTo=7-5rqN-vtlkq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualEmbedder():\n",
    "    def __init__(self, cfg_path=\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\", min_boxes=10, max_boxes=100):\n",
    "        self.cfg_path = cfg_path\n",
    "        #In order to get the box features for the best few proposals and limit the sequence length, we set minimum and maximum boxes and pick those box features.\n",
    "        self.min_boxes = min_boxes\n",
    "        self.max_boxes = max_boxes\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ## Load Config and Model Weights\n",
    "    # I am using the MaskRCNN ResNet-101 FPN checkpoint, but you can use any checkpoint of your preference. This checkpoint is pre-trained on the COCO dataset. \n",
    "    # You can check other checkpoints/configs on the Model Zoo (https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md) page.\n",
    "\n",
    "    def load_config_and_model_weights(self,cfg_path):\n",
    "        cfg = get_cfg()\n",
    "        cfg.merge_from_file(model_zoo.get_config_file(cfg_path))\n",
    "\n",
    "        # ROI HEADS SCORE THRESHOLD\n",
    "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "        # Comment the next line if you're using 'cuda'\n",
    "        # cfg['MODEL']['DEVICE']='cpu'\n",
    "\n",
    "        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(cfg_path)\n",
    "\n",
    "        return cfg\n",
    "\n",
    "    ## Load the Object Detection Model\n",
    "    # The `build_model` method can be used to load a model from the configuration, the checkpoints have to be loaded using the `DetetionCheckpointer`.\n",
    "    def get_model(self,cfg):\n",
    "        # build model\n",
    "        model = build_model(cfg)\n",
    "\n",
    "        # load weights\n",
    "        checkpointer = DetectionCheckpointer(model)\n",
    "        checkpointer.load(cfg.MODEL.WEIGHTS)\n",
    "\n",
    "        # eval mode\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    ## Convert Image to Model Input\n",
    "    # The detectron uses resizing and normalization based on the configuration parameters and the input is to be provided using `ImageList`. \n",
    "    # The `model.backbone.size_divisibility` handles the sizes (padding) such that the FPN lateral and output convolutional features have same dimensions.\n",
    "    def prepare_image_inputs(self,cfg, img_list, model):\n",
    "        # Resizing the image according to the configuration\n",
    "        transform_gen = T.ResizeShortestEdge(\n",
    "                    [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "                )\n",
    "        img_list = [transform_gen.get_transform(img).apply_image(img) for img in img_list]\n",
    "\n",
    "        # Convert to C,H,W format\n",
    "        convert_to_tensor = lambda x: torch.Tensor(x.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "        batched_inputs = [{\"image\":convert_to_tensor(img), \"height\": img.shape[0], \"width\": img.shape[1]} for img in img_list]\n",
    "\n",
    "        # Normalizing the image\n",
    "        num_channels = len(cfg.MODEL.PIXEL_MEAN)\n",
    "        pixel_mean = torch.Tensor(cfg.MODEL.PIXEL_MEAN).view(num_channels, 1, 1)\n",
    "        pixel_std = torch.Tensor(cfg.MODEL.PIXEL_STD).view(num_channels, 1, 1)\n",
    "        normalizer = lambda x: (x - pixel_mean) / pixel_std\n",
    "        images = [normalizer(x[\"image\"]) for x in batched_inputs]\n",
    "\n",
    "        # Convert to ImageList\n",
    "        images =  ImageList.from_tensors(images,model.backbone.size_divisibility)\n",
    "        \n",
    "        return images, batched_inputs\n",
    "\n",
    "    ## Get ResNet+FPN features\n",
    "    # The ResNet model in combination with FPN generates five features for an image at different levels of complexity. \n",
    "    # For more details, refer to the FPN paper or this [article](https://medium.com/@hirotoschwert/digging-into-detectron-2-47b2e794fabd). \n",
    "    # For this tutorial, just know that `p2`, `p3`, `p4`, `p5`, `p6` are the features needed by the RPN (Region Proposal Network). \n",
    "    # The proposals in combination with `p2`, `p3`, `p4`, `p5` are then used by the ROI (Region of Interest) heads to generate box predictions.\n",
    "    def get_features(self,model, images):\n",
    "        features = model.backbone(images.tensor.to(self.device))\n",
    "        return features\n",
    "\n",
    "    ## Get region proposals from RPN\n",
    "    # This RPN takes in the features and images and generates the proposals. Based on the configuration we chose, we get 1000 proposals.\n",
    "    def get_proposals(self,model, images, features):\n",
    "        proposals, _ = model.proposal_generator(images, features)\n",
    "        return proposals\n",
    "\n",
    "    ## Get Box Features for the proposals\n",
    "    # The proposals and features are then used by the ROI heads to get the predictions. In this case, the partial execution of layers becomes significant. \n",
    "    # We want the `box_features` to be the `fc2` outputs of the regions. Hence, I use only the layers that are needed until that step. \n",
    "    def get_box_features(self,model, features, proposals):\n",
    "        features_list = [features[f] for f in ['p2', 'p3', 'p4', 'p5']]\n",
    "        box_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "        box_features = model.roi_heads.box_head.flatten(box_features)\n",
    "        box_features = model.roi_heads.box_head.fc1(box_features)\n",
    "        box_features = model.roi_heads.box_head.fc_relu1(box_features)\n",
    "        box_features = model.roi_heads.box_head.fc2(box_features)\n",
    "\n",
    "        box_features = box_features.reshape(self.num_image, 1000, 1024) # depends on your config and batch size\n",
    "        return box_features, features_list\n",
    "\n",
    "    ## Get prediction logits and boxes\n",
    "    # The prediction class logits and the box predictions from the ROI heads, this is used in the next step to get the boxes and scores from the `FastRCNNOutputs`\n",
    "    def get_prediction_logits(self,model, features_list, proposals):\n",
    "        cls_features = model.roi_heads.box_pooler(features_list, [x.proposal_boxes for x in proposals])\n",
    "        cls_features = model.roi_heads.box_head(cls_features)\n",
    "        pred_class_logits, pred_proposal_deltas = model.roi_heads.box_predictor(cls_features)\n",
    "        return pred_class_logits, pred_proposal_deltas\n",
    "\n",
    "    ## Get FastRCNN scores and boxes\n",
    "    # This results in the softmax scores and the boxes.\n",
    "    def get_box_scores(self,cfg, pred_class_logits, pred_proposal_deltas,proposals):\n",
    "        box2box_transform = Box2BoxTransform(weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n",
    "        smooth_l1_beta = cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA\n",
    "\n",
    "        outputs = FastRCNNOutputs(\n",
    "            box2box_transform,\n",
    "            pred_class_logits,\n",
    "            pred_proposal_deltas,\n",
    "            proposals,\n",
    "            smooth_l1_beta,\n",
    "        )\n",
    "\n",
    "        boxes = outputs.predict_boxes()\n",
    "        scores = outputs.predict_probs()\n",
    "        image_shapes = outputs.image_shapes\n",
    "\n",
    "        return boxes, scores, image_shapes\n",
    "\n",
    "    ## Rescale the boxes to original image size\n",
    "    # We want to rescale the boxes to original size as this is done in the detectron2 library. \n",
    "    # This is done for sanity and to keep it similar to the visualbert repository.\n",
    "    def get_output_boxes(self,boxes, batched_inputs, image_size):\n",
    "        proposal_boxes = boxes.reshape(-1, 4).clone()\n",
    "        scale_x, scale_y = (batched_inputs[\"width\"] / image_size[1], batched_inputs[\"height\"] / image_size[0])\n",
    "        output_boxes = Boxes(proposal_boxes)\n",
    "\n",
    "        output_boxes.scale(scale_x, scale_y)\n",
    "        output_boxes.clip(image_size)\n",
    "\n",
    "        return output_boxes\n",
    "\n",
    "    ## Select the Boxes using NMS\n",
    "    # We need two thresholds - NMS threshold for the NMS box section, and score threshold for the score based section.\n",
    "    # First NMS is performed for all the classes and the max scores of each proposal box and each class is updated.\n",
    "    # Then the class score threshold is used to select the boxes from those.\n",
    "    def select_boxes(self,cfg, output_boxes, scores):\n",
    "        test_score_thresh = cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST\n",
    "        test_nms_thresh = cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST\n",
    "        cls_prob = scores.detach()\n",
    "        cls_boxes = output_boxes.tensor.detach().reshape(1000,80,4)\n",
    "        max_conf = torch.zeros((cls_boxes.shape[0]))\n",
    "        for cls_ind in range(0, cls_prob.shape[1]-1):\n",
    "            cls_scores = cls_prob[:, cls_ind+1].cpu()\n",
    "            det_boxes = cls_boxes[:,cls_ind,:].cpu()\n",
    "            keep = np.array(nms(det_boxes, cls_scores, test_nms_thresh))\n",
    "            max_conf[keep] = torch.where(cls_scores[keep] > max_conf[keep], cls_scores[keep], max_conf[keep])\n",
    "        keep_boxes = torch.where(max_conf >= test_score_thresh)[0]\n",
    "        return keep_boxes, max_conf\n",
    "\n",
    "    ## Limit the total number of boxes\n",
    "    # In order to get the box features for the best few proposals and limit the sequence length, we set minimum and maximum boxes and pick those box features.\n",
    "    def filter_boxes(self,keep_boxes, max_conf, min_boxes, max_boxes):\n",
    "        if len(keep_boxes) < min_boxes:\n",
    "            keep_boxes = np.argsort(max_conf).numpy()[::-1][:min_boxes]\n",
    "        elif len(keep_boxes) > max_boxes:\n",
    "            keep_boxes = np.argsort(max_conf).numpy()[::-1][:max_boxes]\n",
    "        return keep_boxes\n",
    "\n",
    "    ## Get the visual embeddings :) \n",
    "    # Finally, the boxes are chosen using the `keep_boxes` indices and from the `box_features` tensor.\n",
    "    def get_visual_embeds(self,box_features, keep_boxes):\n",
    "        return box_features[keep_boxes.copy()]\n",
    "    \n",
    "    def visual_embeds_detectron2(self, img_list):\n",
    "        self.num_image=len(img_list)\n",
    "        cfg = self.load_config_and_model_weights(self.cfg_path)\n",
    "        model = self.get_model(cfg)\n",
    "        images, batched_inputs = self.prepare_image_inputs(cfg, img_list, model)\n",
    "        features = self.get_features(model, images)\n",
    "        print(features.keys())\n",
    "        proposals = self.get_proposals(model, images, features)\n",
    "        box_features, features_list = self.get_box_features(model, features, proposals)\n",
    "        pred_class_logits, pred_proposal_deltas = self.get_prediction_logits(model, features_list, proposals)\n",
    "        boxes, scores, image_shapes = self.get_box_scores(cfg, pred_class_logits, pred_proposal_deltas, proposals)\n",
    "        output_boxes = [self.get_output_boxes(boxes[i], batched_inputs[i], proposals[i].image_size) for i in range(len(proposals))]\n",
    "        temp = [self.select_boxes(cfg, output_boxes[i], scores[i]) for i in range(len(scores))]\n",
    "        keep_boxes, max_conf = [],[]\n",
    "        for keep_box, mx_conf in temp:\n",
    "            keep_boxes.append(keep_box)\n",
    "            max_conf.append(mx_conf)\n",
    "        keep_boxes = [self.filter_boxes(keep_box, mx_conf, self.min_boxes, self.max_boxes) for keep_box, mx_conf in zip(keep_boxes, max_conf)]\n",
    "        visual_embeds = [self.get_visual_embeds(box_feature, keep_box) for box_feature, keep_box in zip(box_features, keep_boxes)]\n",
    "        return visual_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='../data/hateful_memes/train_df_wQuery_.jsonl'\n",
    "import pandas as pd\n",
    "img_data = pd.read_json(path_or_buf=data_path, lines=True).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500\n",
      "dict_keys(['id', 'img', 'label', 'text', 'query_1'])\n"
     ]
    }
   ],
   "source": [
    "print(len(img_data))\n",
    "print(img_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_path=\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
    "MIN_BOXES=10 \n",
    "MAX_BOXES=100\n",
    "visualembedder=VisualEmbedder(cfg_path=cfg_path, min_boxes=MIN_BOXES, max_boxes=MAX_BOXES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[-3.6105, -2.2630,  2.2598,  ..., -2.5526,  0.3378,  1.5410],\n",
       "         [ 2.1424,  1.1499,  0.4670,  ..., -0.5727,  0.6574,  0.3634],\n",
       "         [ 1.9217, -0.5865, -0.4217,  ...,  0.8373,  1.3967,  1.7566],\n",
       "         ...,\n",
       "         [ 1.5050,  1.8730,  1.2205,  ..., -1.2324,  1.6751, -0.4564],\n",
       "         [-1.5518,  0.5050, -0.0539,  ..., -1.0867,  1.0135,  0.4631],\n",
       "         [ 0.2506, -0.3850, -0.0968,  ..., -1.3164,  1.0680,  0.9516]],\n",
       "        device='cuda:0', grad_fn=<IndexBackward0>)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "img_inpainted_dir='../data/hateful_memes/img_inpainted'\n",
    "visualembedder.visual_embeds_detectron2([cv2.imread(os.path.join(img_inpainted_dir, img_data[3]['img'].split('/')[-1]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HateMemeDataset(Dataset):\n",
    "    def __init__(self, data_path, img_dir, target_transform=None):\n",
    "        self.img_data = pd.read_json(path_or_buf=data_path, lines=True).to_dict(orient='records')\n",
    "        self.img_dir = img_dir\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_embed=self.img_data[idx]['visual_embedding']\n",
    "        img_text=self.img_data[idx]['text']\n",
    "        img_query=self.img_data[idx]['query_1']\n",
    "        label=self.img_data[idx]['label']\n",
    "        data_id=self.img_data[idx]['id']\n",
    "        \n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image_embed, label, img_text, img_query, data_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_data_path='../data/hateful_memes/train_df_wQuery_.jsonl'\n",
    "validation_data_path='../data/hateful_memes/dev_seen_df_wQuery_.jsonl'\n",
    "img_inpainted_dir='../data/hateful_memes/img_inpainted'\n",
    "cfg_path=\"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\"\n",
    "MIN_BOXES=10 \n",
    "MAX_BOXES=100\n",
    "\n",
    "visualembedder=VisualEmbedder(cfg_path=cfg_path, min_boxes=MIN_BOXES, max_boxes=MAX_BOXES)\n",
    "\n",
    "training_data = HateMemeDataset(train_data_path, img_inpainted_dir, visualembedder, target_transform=None)\n",
    "validation_data = HateMemeDataset(validation_data_path, img_inpainted_dir,visualembedder, target_transform=None)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(validation_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['p2', 'p3', 'p4', 'p5', 'p6'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mproposal_generator.anchor_generator.cell_anchors.{0, 1, 2, 3, 4}\u001b[0m\n",
      "/home/cchang/miniconda3/envs/visualbert/lib/python3.10/site-packages/detectron2/layers/wrappers.py:84: UserWarning: Plan failed with an OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU  (Triggered internally at /opt/conda/conda-bld/pytorch_1712608839953/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:924.)\n",
      "  x = F.conv2d(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 60.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Display image and label.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_features, train_labels, train_text, train_query, train_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_features\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_labels\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[67], line 21\u001b[0m, in \u001b[0;36mHateMemeDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     20\u001b[0m     image_file_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_data[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 21\u001b[0m     image_embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualembedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual_embeds_detectron2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_file_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# image_embed=self.visual_embeds[idx]\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     img_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_data[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[62], line 175\u001b[0m, in \u001b[0;36mVisualEmbedder.visual_embeds_detectron2\u001b[0;34m(self, img_list)\u001b[0m\n\u001b[1;32m    173\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model(cfg)\n\u001b[1;32m    174\u001b[0m images, batched_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_image_inputs(cfg, img_list, model)\n\u001b[0;32m--> 175\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(features\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    177\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_proposals(model, images, features)\n",
      "Cell \u001b[0;32mIn[62], line 74\u001b[0m, in \u001b[0;36mVisualEmbedder.get_features\u001b[0;34m(self, model, images)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_features\u001b[39m(\u001b[38;5;28mself\u001b[39m,model, images):\n\u001b[0;32m---> 74\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/detectron2/modeling/backbone/fpn.py:126\u001b[0m, in \u001b[0;36mFPN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    114\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m        input (dict[str->Tensor]): mapping feature map name (e.g., \"res5\") to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m            [\"p2\", \"p3\", ..., \"p6\"].\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     bottom_up_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbottom_up\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    128\u001b[0m     prev_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlateral_convs[\u001b[38;5;241m0\u001b[39m](bottom_up_features[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/detectron2/modeling/backbone/resnet.py:449\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    447\u001b[0m     outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, stage \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_names, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages):\n\u001b[0;32m--> 449\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_out_features:\n\u001b[1;32m    451\u001b[0m         outputs[name] \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/detectron2/modeling/backbone/resnet.py:201\u001b[0m, in \u001b[0;36mBottleneckBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[1;32m    199\u001b[0m out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu_(out)\n\u001b[0;32m--> 201\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m     shortcut \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortcut(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/detectron2/layers/wrappers.py:88\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     84\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m     85\u001b[0m     x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/detectron2/layers/batch_norm.py:57\u001b[0m, in \u001b[0;36mFrozenBatchNorm2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m scale\u001b[38;5;241m.\u001b[39mto(out_dtype) \u001b[38;5;241m+\u001b[39m bias\u001b[38;5;241m.\u001b[39mto(out_dtype)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# When gradients are not needed, F.batch_norm is a single fused op\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# and provide more optimization opportunities.\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/visualbert/lib/python3.10/site-packages/torch/nn/functional.py:2509\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2507\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2510\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 60.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels, train_text, train_query, train_id = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
